export type ImpactClass = "Foundational" | "Breakthrough" | "Optimization" | "Production Innovation";

export interface Paper {
  id: string;
  title: string;
  year: number;
  difficulty: "Beginner" | "Intermediate" | "Advanced";
  authors: string;
  institution: string;
  citations: number;
  section: "foundation" | "improvement" | "modern";
  whyItMatters: string;
  problem: string;
  coreContribution: string;
  methodIntuition: string;
  results: string;
  strengths: string;
  limitations: string;
  tldr: string;
  keyInsight: string;
  whoShouldRead: string[];
  link: string;
  concepts: string[];
  impact: ImpactClass;
}

type PaperTopic = {
  keywords: string[];
  papers: Paper[];
};

function makePaper(
  base: Omit<Paper, "authors" | "institution" | "citations" | "section" | "strengths" | "limitations" | "whoShouldRead" | "concepts" | "impact"> & {
    authors?: string;
    institution?: string;
    citations?: number;
    section?: Paper["section"];
    strengths?: string;
    limitations?: string;
    whoShouldRead?: string[];
    concepts?: string[];
    impact?: ImpactClass;
  }
): Paper {
  return {
    authors: "Unknown",
    institution: "Various",
    citations: 500,
    section: "foundation",
    strengths: "Well-established method with strong theoretical grounding.",
    limitations: "May not scale to extremely large datasets without modifications.",
    whoShouldRead: ["beginner", "researcher", "interview prep"],
    concepts: [],
    impact: "Foundational",
    ...base,
  };
}
// Concept and impact classification mappings
const paperMeta: Record<string, { concepts: string[]; impact: ImpactClass }> = {
  t1: { concepts: ["Attention Mechanisms", "Sequence Modeling", "Parallelization"], impact: "Foundational" },
  t2: { concepts: ["Transfer Learning", "Pre-training", "NLP"], impact: "Breakthrough" },
  t3: { concepts: ["Few-Shot Learning", "Scaling", "In-Context Learning"], impact: "Breakthrough" },
  t4: { concepts: ["Positional Encoding", "Attention Mechanisms", "Length Generalization"], impact: "Optimization" },
  t5: { concepts: ["GPU Optimization", "Attention Mechanisms", "Systems Engineering"], impact: "Production Innovation" },
  t6: { concepts: ["Fine-Tuning", "Parameter Efficiency", "Transfer Learning"], impact: "Production Innovation" },
  t7: { concepts: ["Scaling Laws", "Training Efficiency", "Empirical ML"], impact: "Foundational" },
  t8: { concepts: ["Positional Encoding", "Attention Mechanisms", "Rotary Embeddings"], impact: "Optimization" },
  nn1: { concepts: ["CNNs", "GPU Training", "Image Classification"], impact: "Foundational" },
  nn2: { concepts: ["Residual Learning", "Deep Networks", "Optimization"], impact: "Breakthrough" },
  nn3: { concepts: ["Normalization", "Training Stability", "Regularization"], impact: "Foundational" },
  nn4: { concepts: ["Weight Initialization", "Gradient Flow", "Training Stability"], impact: "Optimization" },
  nn5: { concepts: ["Normalization", "Transformers", "Sequence Modeling"], impact: "Optimization" },
  rf1: { concepts: ["Ensemble Methods", "Decision Trees", "Bagging"], impact: "Foundational" },
  rf2: { concepts: ["Boosting", "Ensemble Methods", "Gradient Descent"], impact: "Breakthrough" },
  rf3: { concepts: ["Boosting", "Systems Engineering", "Scalability"], impact: "Production Innovation" },
  rf4: { concepts: ["Interpretability", "Feature Importance", "Model Explanation"], impact: "Breakthrough" },
  rf5: { concepts: ["AutoML", "Hyperparameter Tuning", "Ensemble Methods"], impact: "Production Innovation" },
  rl1: { concepts: ["Deep RL", "Q-Learning", "Game AI"], impact: "Breakthrough" },
  rl2: { concepts: ["Policy Gradient", "Actor-Critic", "Continuous Control"], impact: "Foundational" },
  rl3: { concepts: ["Model-Based RL", "Planning", "Sample Efficiency"], impact: "Optimization" },
  rl4: { concepts: ["Multi-Agent RL", "Game Theory", "Self-Play"], impact: "Breakthrough" },
  rl5: { concepts: ["Offline RL", "Batch Learning", "Safety"], impact: "Production Innovation" },
  cl1: { concepts: ["K-Means", "Centroid Methods", "Unsupervised Learning"], impact: "Foundational" },
  cl2: { concepts: ["Density Estimation", "Anomaly Detection", "Spatial Analysis"], impact: "Foundational" },
  cl3: { concepts: ["Spectral Methods", "Graph Theory", "Dimensionality Reduction"], impact: "Optimization" },
  cl4: { concepts: ["Hierarchical Clustering", "Density Methods", "Scalability"], impact: "Production Innovation" },
  cl5: { concepts: ["Dimensionality Reduction", "Manifold Learning", "Visualization"], impact: "Foundational" },
};

// Post-process papers with metadata
function enrichPaper(p: Paper): Paper {
  const meta = paperMeta[p.id];
  if (meta) {
    return { ...p, concepts: meta.concepts, impact: meta.impact };
  }
  return p;
}

const allTopics: PaperTopic[] = [
  {
    keywords: ["transformer", "attention", "bert", "gpt", "self-attention"],
    papers: [
      makePaper({ id: "t1", title: "Attention Is All You Need", year: 2017, difficulty: "Intermediate", authors: "Vaswani, Shazeer, Parmar et al.", institution: "Google Brain", citations: 110000, section: "foundation", whyItMatters: "Replaced recurrent architectures with pure attention, enabling massive parallelization and modern AI.", problem: "RNNs were slow due to sequential computation and struggled with long-range dependencies.", coreContribution: "The Transformer — a model built entirely on self-attention with positional encodings.", methodIntuition: "Instead of reading word-by-word, see how every word relates to every other word simultaneously. Multi-head attention reads the same sentence through multiple lenses.", results: "SOTA BLEU on English-German (28.4) and English-French (41.8), training significantly faster.", strengths: "Fully parallelizable, scales to billions of parameters, captures global context.", limitations: "Quadratic memory in sequence length; requires positional encoding hacks.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "Replaced RNNs with pure attention. Enabled parallel training. Foundation for GPT, BERT, and modern AI.", keyInsight: "Attention(Q,K,V) = softmax(QK^T/√d)V — this single equation powers most of modern AI.", link: "https://arxiv.org/abs/1706.03762" }),
      makePaper({ id: "t2", title: "BERT: Pre-training of Deep Bidirectional Transformers", year: 2018, difficulty: "Intermediate", authors: "Devlin, Chang, Lee, Toutanova", institution: "Google AI Language", citations: 85000, section: "improvement", whyItMatters: "Showed bidirectional pre-training creates representations that transfer remarkably well to downstream NLP tasks.", problem: "Previous language models were unidirectional, missing context from future tokens.", coreContribution: "Masked Language Modeling (MLM) and Next Sentence Prediction for bidirectional pre-training.", methodIntuition: "Fill-in-the-blank exercises: predict randomly masked words using both left and right context.", results: "SOTA on 11 NLP benchmarks including GLUE and SQuAD with minimal task-specific modifications.", strengths: "Simple fine-tuning paradigm, strong transfer learning, bidirectional context.", limitations: "Pre-training is expensive; MLM creates train-test discrepancy (no [MASK] at inference).", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "Bidirectional pre-training + simple fine-tuning = SOTA on almost every NLP task.", keyInsight: "Masked LM forces truly bidirectional understanding — the model must use ALL surrounding context.", link: "https://arxiv.org/abs/1810.04805" }),
      makePaper({ id: "t3", title: "GPT-3: Language Models are Few-Shot Learners", year: 2020, difficulty: "Advanced", authors: "Brown, Mann, Ryder et al.", institution: "OpenAI", citations: 35000, section: "modern", whyItMatters: "Demonstrated that 175B parameters enable few-shot learning without fine-tuning.", problem: "Fine-tuning requires labeled data for each task. Can a single model generalize from a few prompt examples?", coreContribution: "Showed emergent in-context learning abilities at scale across diverse tasks.", methodIntuition: "A student who has read the entire internet doesn't need teaching from scratch — just show a few examples.", results: "Near SOTA on many benchmarks with zero/few examples. Generated coherent articles, translated, wrote code.", strengths: "No gradient updates needed, versatile, emergent abilities.", limitations: "Enormous compute cost, potential biases, hallucination issues.", whoShouldRead: ["researcher", "interview prep"], tldr: "Scale enough and the model learns to learn from in-context examples. Launched prompt engineering.", keyInsight: "Emergent abilities appear at scale — capabilities suddenly appear when crossing certain parameter thresholds.", link: "https://arxiv.org/abs/2005.14165" }),
      makePaper({ id: "t4", title: "Attention with Linear Biases (ALiBi)", year: 2022, difficulty: "Intermediate", authors: "Press, Smith, Lewis", institution: "Meta AI", citations: 1200, section: "improvement", whyItMatters: "Showed that learned positional encodings aren't necessary — simple linear biases enable length generalization.", problem: "Sinusoidal and learned positional encodings fail when sequences exceed training lengths.", coreContribution: "Replace positional encodings with linear biases that penalize attention scores by distance.", methodIntuition: "Instead of encoding WHERE a token is, penalize attention between distant tokens — closer words naturally attend more.", results: "Enabled models trained on 1024 tokens to generalize to 2048+ without quality loss.", strengths: "No extra parameters, simple implementation, length generalization.", limitations: "Slight performance gap on some tasks vs. rotary embeddings.", whoShouldRead: ["researcher", "interview prep"], tldr: "Drop positional encodings, add linear distance penalties. Simpler and generalizes to longer sequences.", keyInsight: "Position information doesn't need to be injected — it can be inferred from attention decay patterns.", link: "https://arxiv.org/abs/2108.12409" }),
      makePaper({ id: "t5", title: "FlashAttention: Fast and Memory-Efficient Exact Attention", year: 2022, difficulty: "Advanced", authors: "Dao, Fu, Ermon et al.", institution: "Stanford", citations: 3500, section: "modern", whyItMatters: "Made transformers 2-4x faster by rewriting attention for GPU memory hierarchy, enabling longer sequences.", problem: "Standard attention is quadratic in memory — O(N²) — making long sequences impractical.", coreContribution: "Tiling and recomputation to avoid materializing the full N×N attention matrix in GPU HBM.", methodIntuition: "Instead of computing the entire attention matrix at once, compute it in small tiles that fit in fast SRAM. Like reading a book chapter by chapter instead of loading the whole book into working memory.", results: "2-4x wall-clock speedup, 5-20x memory reduction. Enabled training with context lengths of 16K-64K.", strengths: "Exact computation (not approximate), massive memory savings, easy drop-in replacement.", limitations: "Complex CUDA implementation, hardware-specific optimizations needed.", whoShouldRead: ["researcher"], tldr: "Rewrite attention to respect GPU memory hierarchy. Exact same math, dramatically faster. Unlocked long-context LLMs.", keyInsight: "The bottleneck is memory bandwidth, not compute. By reducing HBM reads/writes through tiling, attention becomes compute-bound instead of memory-bound.", link: "https://arxiv.org/abs/2205.14135" }),
      makePaper({ id: "t6", title: "LoRA: Low-Rank Adaptation of Large Language Models", year: 2021, difficulty: "Intermediate", authors: "Hu, Shen, Wallis et al.", institution: "Microsoft", citations: 12000, section: "modern", whyItMatters: "Made fine-tuning giant models accessible by training only small rank-decomposition matrices instead of all parameters.", problem: "Full fine-tuning of LLMs requires storing separate copies of billion-parameter models for each task.", coreContribution: "Freeze pretrained weights, inject trainable low-rank matrices into each layer: W + BA where B,A are small.", methodIntuition: "Instead of remodeling an entire building for a new tenant, just add modular furniture. LoRA adds lightweight adapters without changing the foundation.", results: "Matched full fine-tuning performance with 10,000x fewer trainable parameters. Enabled fine-tuning on single GPUs.", strengths: "Tiny storage per task, no inference latency increase, composable adapters.", limitations: "May underperform full fine-tuning on highly specialized domains.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "Fine-tune LLMs by training tiny low-rank matrices. Same quality, fraction of the cost. Democratized LLM customization.", keyInsight: "Weight updates during fine-tuning have low intrinsic rank — you don't need to update all parameters because the adaptation lives in a low-dimensional subspace.", link: "https://arxiv.org/abs/2106.09685" }),
      makePaper({ id: "t7", title: "Scaling Laws for Neural Language Models", year: 2020, difficulty: "Advanced", authors: "Kaplan, McCandlish, Henighan et al.", institution: "OpenAI", citations: 5500, section: "foundation", whyItMatters: "Revealed predictable power-law relationships between model size, data, compute, and performance — enabling principled scaling decisions.", problem: "How should we allocate compute budget between model size and training data?", coreContribution: "Empirical scaling laws showing loss decreases as a power law with model parameters, dataset size, and compute.", methodIntuition: "Like physics laws: doubling model size gives a predictable performance improvement. You can forecast how good a model will be before training it.", results: "Guided the development of GPT-3 and subsequent LLMs. Showed that bigger models are more sample-efficient.", strengths: "Predictive, guides resource allocation, reduces expensive trial-and-error.", limitations: "May not hold across all architectures; doesn't capture emergent abilities.", whoShouldRead: ["researcher"], tldr: "Performance follows predictable power laws with scale. Bigger models are more efficient learners. The roadmap for modern AI scaling.", keyInsight: "Model performance is a smooth function of compute — there are no sharp phase transitions in loss (though capabilities can emerge discontinuously).", link: "https://arxiv.org/abs/2001.08361" }),
      makePaper({ id: "t8", title: "RoFormer: Enhanced Transformer with Rotary Position Embedding", year: 2023, difficulty: "Advanced", authors: "Su, Lu, Pan et al.", institution: "Zhuiyi Technology", citations: 2800, section: "modern", whyItMatters: "Rotary Position Embeddings (RoPE) became the standard for modern LLMs including LLaMA, providing elegant relative position encoding.", problem: "How to encode position such that attention naturally captures relative distances without absolute position injection?", coreContribution: "Encode positions by rotating query/key vectors — the dot product then depends only on relative position through rotation angle differences.", methodIntuition: "Spin each token's vector by an angle proportional to its position. When two tokens attend to each other, only the angle DIFFERENCE matters — giving you relative positioning for free.", results: "Adopted by LLaMA, Mistral, and most modern open-source LLMs. Better length generalization than learned embeddings.", strengths: "Elegant math, captures relative position naturally, no extra parameters.", limitations: "Requires careful frequency base tuning for long-context extension.", whoShouldRead: ["researcher", "interview prep"], tldr: "Rotate vectors by position, get relative position encoding for free. Elegant, efficient, now standard in open-source LLMs.", keyInsight: "The inner product of rotated vectors depends only on the rotation difference — position becomes a relative concept, just as it should be in language.", link: "https://arxiv.org/abs/2104.09864" }),
    ],
  },
  {
    keywords: ["neural network", "deep learning", "backpropagation", "perceptron", "mlp"],
    papers: [
      makePaper({ id: "nn1", title: "ImageNet Classification with Deep Convolutional Neural Networks (AlexNet)", year: 2012, difficulty: "Beginner", authors: "Krizhevsky, Sutskever, Hinton", institution: "University of Toronto", citations: 120000, section: "foundation", whyItMatters: "Triggered the deep learning revolution by winning ImageNet 2012 by a massive margin.", problem: "Traditional CV relied on hand-crafted features. Could learned features outperform decades of engineering?", coreContribution: "Deep CNN on GPUs with ReLU, dropout, and data augmentation — now standard techniques.", methodIntuition: "Let the network learn patterns layer by layer — edges first, textures next, objects last.", results: "15.3% top-5 error vs 26.2% for runner-up — an unprecedented 10.8pp improvement.", strengths: "Proved deep learning works at scale, established GPU training paradigm.", limitations: "Large model for its time, required significant engineering effort.", whoShouldRead: ["beginner", "interview prep"], tldr: "Deep CNNs + GPUs demolished hand-crafted features. Kicked off the deep learning era.", keyInsight: "ReLU (max(0,x)) solved vanishing gradients and trained 6x faster than tanh.", link: "https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b" }),
      makePaper({ id: "nn2", title: "Deep Residual Learning for Image Recognition (ResNet)", year: 2015, difficulty: "Intermediate", authors: "He, Zhang, Ren, Sun", institution: "Microsoft Research", citations: 180000, section: "foundation", whyItMatters: "Skip connections became the most important architectural innovation since convolutions.", problem: "Adding layers paradoxically made networks perform WORSE — from optimization difficulty, not overfitting.", coreContribution: "Residual connections: output = F(x) + x, letting gradients flow directly through.", methodIntuition: "Learn the difference from input instead of the full transformation. If a layer isn't useful, learn F(x)=0.", results: "152 layers, 3.57% error — surpassing human performance on ImageNet.", strengths: "Enables arbitrarily deep networks, simple to implement, universal applicability.", limitations: "Skip connections add memory overhead; diminishing returns beyond certain depth.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "Skip connections = deep networks that actually train. Simple idea, revolutionary impact.", keyInsight: "F(x) + x makes adding layers risk-free: if a layer isn't useful, it can learn the identity.", link: "https://arxiv.org/abs/1512.03385" }),
      makePaper({ id: "nn3", title: "Batch Normalization: Accelerating Deep Network Training", year: 2015, difficulty: "Beginner", authors: "Ioffe, Szegedy", institution: "Google", citations: 45000, section: "foundation", whyItMatters: "Made training deep networks dramatically faster and more stable.", problem: "Internal covariate shift made training slow and fragile, requiring careful initialization.", coreContribution: "Normalizing layer inputs within each mini-batch with learnable scale and shift parameters.", methodIntuition: "Standardize the 'language' between layers so each gets consistent inputs and learns faster.", results: "Same accuracy in 14x fewer steps. Enabled higher learning rates and less sensitivity to init.", strengths: "Near-universal improvement, enables higher learning rates, acts as regularizer.", limitations: "Depends on batch size; behaves differently at train vs test time.", whoShouldRead: ["beginner", "interview prep"], tldr: "Normalize each layer's inputs during training. Train faster, worry less about initialization.", keyInsight: "Learnable γ and β let the network undo normalization if needed — never reduces model capacity.", link: "https://arxiv.org/abs/1502.03167" }),
      makePaper({ id: "nn4", title: "Xavier and Kaiming Initialization for Deep Networks", year: 2015, difficulty: "Intermediate", authors: "He, Zhang, Ren, Sun / Glorot, Bengio", institution: "Microsoft Research / Université de Montréal", citations: 22000, section: "improvement", whyItMatters: "Proper weight initialization prevents gradients from exploding or vanishing at the start of training.", problem: "Random initialization often causes signal to shrink or blow up as it passes through layers.", coreContribution: "Initialize weights with variance calibrated to layer width: Var(w) = 2/n for ReLU (Kaiming).", methodIntuition: "Set the initial volume of each layer's output to match its input. Too quiet and signals die; too loud and they explode.", results: "Enabled training of very deep networks without batch normalization. Standard in all frameworks.", strengths: "Simple, principled, universally applicable.", limitations: "Derived for specific activations; may need adjustment for novel architectures.", whoShouldRead: ["beginner", "interview prep"], tldr: "Set initial weight variance to preserve signal magnitude through layers. Small detail, huge impact.", keyInsight: "For ReLU, Var(w) = 2/fan_in because ReLU zeros out half the outputs, so you need 2x the variance to compensate.", link: "https://arxiv.org/abs/1502.01852" }),
      makePaper({ id: "nn5", title: "Layer Normalization", year: 2016, difficulty: "Intermediate", authors: "Ba, Kiros, Hinton", institution: "University of Toronto", citations: 18000, section: "improvement", whyItMatters: "LayerNorm normalizes across features instead of batch, making it ideal for transformers and RNNs where batch statistics are unreliable.", problem: "BatchNorm depends on batch size and doesn't work well with variable-length sequences or small batches.", coreContribution: "Normalize across the feature dimension for each individual example, independent of other examples in the batch.", methodIntuition: "Instead of comparing yourself to classmates (batch norm), compare your own features against each other. Works the same whether you're alone or in a crowd.", results: "Became the standard normalization in transformers, enabling stable training of LLMs.", strengths: "Batch-size independent, works with any sequence length, identical train/test behavior.", limitations: "Slightly less effective than BatchNorm for CNNs in some cases.", whoShouldRead: ["researcher", "interview prep"], tldr: "Normalize per-example across features instead of per-feature across batch. Essential for transformers.", keyInsight: "LayerNorm's batch-independence is why it works in autoregressive generation — you can't batch-normalize when generating one token at a time.", link: "https://arxiv.org/abs/1607.06450" }),
      makePaper({ id: "nn6", title: "EfficientNet: Rethinking Model Scaling for CNNs", year: 2019, difficulty: "Intermediate", authors: "Tan, Le", institution: "Google Brain", citations: 15000, section: "modern", whyItMatters: "Showed that balanced scaling of depth, width, and resolution is far more efficient than scaling any single dimension.", problem: "Models were scaled up arbitrarily — more layers OR more channels OR higher resolution. What's the optimal way to use a larger compute budget?", coreContribution: "Compound scaling: uniformly scale depth, width, and resolution with fixed ratios using a compound coefficient.", methodIntuition: "If you get a bigger screen, you should increase both resolution AND size proportionally, not just stretch one dimension. Same for neural networks.", results: "EfficientNet-B7 achieved SOTA 84.3% top-1 on ImageNet with 8.4x fewer parameters than the best existing models.", strengths: "Principled scaling, extremely parameter-efficient, strong baselines.", limitations: "Neural architecture search for base model is expensive; compound scaling rules are empirical.", whoShouldRead: ["researcher", "interview prep"], tldr: "Scale depth, width, and resolution together with fixed ratios. 8x smaller, better accuracy. Smart scaling beats brute force.", keyInsight: "The compound scaling coefficient φ controls all three dimensions: depth=α^φ, width=β^φ, resolution=γ^φ where α·β²·γ² ≈ 2.", link: "https://arxiv.org/abs/1905.11946" }),
      makePaper({ id: "nn7", title: "Universal Approximation Theorem and Its Implications", year: 1989, difficulty: "Beginner", authors: "Hornik, Stinchcombe, White / Cybenko", institution: "Various", citations: 25000, section: "foundation", whyItMatters: "Proved that neural networks can approximate any continuous function — the theoretical justification for why deep learning works.", problem: "Can neural networks actually represent complex functions, or are they inherently limited in what they can learn?", coreContribution: "A single hidden layer with enough neurons can approximate any continuous function on a compact set to arbitrary precision.", methodIntuition: "A neural network is like modeling clay — with enough material (neurons), you can sculpt any shape (function). The theorem proves there's always enough clay.", results: "Provided the theoretical foundation for neural networks as universal function approximators.", strengths: "Powerful existence proof, applies to any continuous function.", limitations: "Says nothing about HOW MANY neurons or HOW to find the right weights — existence ≠ learnability.", whoShouldRead: ["beginner", "researcher"], tldr: "Neural networks can represent any function. The theory that justifies everything we do in deep learning.", keyInsight: "The theorem guarantees existence but not efficiency — deep networks often need exponentially fewer neurons than shallow ones for the same function.", link: "https://doi.org/10.1016/0893-6080(89)90020-8" }),
      makePaper({ id: "nn8", title: "Mixture of Experts: Conditional Computation at Scale", year: 2022, difficulty: "Advanced", authors: "Fedus, Zoph, Shazeer", institution: "Google Brain", citations: 4500, section: "modern", whyItMatters: "MoE enables training trillion-parameter models while only activating a fraction of parameters per input — the architecture behind GPT-4 and Mixtral.", problem: "Larger models perform better but are proportionally more expensive. Can we increase capacity without increasing per-example compute?", coreContribution: "Route each input to a subset of specialized 'expert' sub-networks, keeping total compute constant while increasing model capacity.", methodIntuition: "A hospital doesn't send every patient to every doctor. A router triages patients to specialists. MoE does this for neural networks — each input activates only relevant experts.", results: "Switch Transformer: 7x faster pre-training than T5 with same compute budget. Enabled trillion-parameter models.", strengths: "Scales model capacity without proportional compute increase, enables specialization.", limitations: "Load balancing across experts is tricky; routing instability during training.", whoShouldRead: ["researcher"], tldr: "Route inputs to specialized experts, only activate a fraction of the model per input. Scale to trillions of parameters efficiently.", keyInsight: "Sparse activation is the key: a 1T parameter MoE model may only use 10B parameters per input, giving the capacity of a giant model with the cost of a small one.", link: "https://arxiv.org/abs/2101.03961" }),
    ],
  },
  {
    keywords: ["reinforcement learning", "rl", "reward", "policy", "q-learning", "agent"],
    papers: [
      makePaper({ id: "rl1", title: "Playing Atari with Deep Reinforcement Learning (DQN)", year: 2013, difficulty: "Intermediate", authors: "Mnih, Kavukcuoglu, Silver et al.", institution: "DeepMind", citations: 25000, section: "foundation", whyItMatters: "First demonstration that deep learning could master complex control tasks from raw pixels.", problem: "RL with function approximation was unstable. Could a deep network play games from pixels?", coreContribution: "Deep Q-Network with experience replay and target network for stable training.", methodIntuition: "Play thousands of games, remember experiences. Learn to predict future reward for each action.", results: "Superhuman on 29/49 Atari games using the same architecture for all games.", strengths: "General-purpose, learns from raw sensory input, no game-specific engineering.", limitations: "Discrete actions only, sample inefficient, overestimates Q-values.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "A single neural network learned dozens of Atari games from raw pixels. Launched deep RL.", keyInsight: "Experience replay breaks temporal correlations — without it, correlated samples cause divergence.", link: "https://arxiv.org/abs/1312.5602" }),
      makePaper({ id: "rl2", title: "Proximal Policy Optimization (PPO)", year: 2017, difficulty: "Intermediate", authors: "Schulman, Wolski, Dhariwal et al.", institution: "OpenAI", citations: 18000, section: "improvement", whyItMatters: "PPO is THE default RL algorithm — it powers RLHF in ChatGPT.", problem: "TRPO was effective but complex. Could we get similar stability with first-order methods?", coreContribution: "Clipped surrogate objective preventing destructively large policy updates.", methodIntuition: "Don't change recipes too drastically between attempts. PPO clips changes to a 'safe zone'.", results: "Matched TRPO across tasks while being much simpler to implement and tune.", strengths: "Simple implementation, stable training, works across domains.", limitations: "Can be sensitive to hyperparameters; sample efficiency could be better.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "Simple clipped objective replaces trust regions. Easy, works everywhere. Powers modern RLHF.", keyInsight: "min(r·A, clip(r, 1-ε, 1+ε)·A) creates a pessimistic bound — only improves when both terms agree.", link: "https://arxiv.org/abs/1707.06347" }),
      makePaper({ id: "rl3", title: "Mastering Go with Deep Neural Networks (AlphaGo)", year: 2016, difficulty: "Advanced", authors: "Silver, Huang, Maddison et al.", institution: "DeepMind", citations: 16000, section: "foundation", whyItMatters: "Go was AI's grand challenge. AlphaGo's victory was a watershed moment.", problem: "Go's branching factor (~250) makes brute-force search impossible.", coreContribution: "Combined policy networks, value networks, and Monte Carlo Tree Search.", methodIntuition: "A grandmaster who intuitively feels which moves look promising AND accurately evaluates positions.", results: "Defeated world champion Lee Sedol 4-1. Later versions beat world #1 Ke Jie 3-0.", strengths: "Combined learning with search, superhuman performance.", limitations: "Requires enormous compute, specific to perfect-information games.", whoShouldRead: ["researcher", "interview prep"], tldr: "Deep learning + tree search conquered Go. Proved AI can develop superhuman intuition.", keyInsight: "Self-play transcended human knowledge — AlphaGo discovered strategies no human conceived.", link: "https://www.nature.com/articles/nature16961" }),
      makePaper({ id: "rl4", title: "Soft Actor-Critic: Off-Policy Maximum Entropy RL", year: 2018, difficulty: "Advanced", authors: "Haarnoja, Zhou, Abbeel, Levine", institution: "UC Berkeley", citations: 8000, section: "improvement", whyItMatters: "SAC introduced entropy maximization to RL, encouraging exploration while maintaining stability. Became standard for continuous control.", problem: "Model-free RL is sample inefficient and brittle. Can we encourage exploration systematically?", coreContribution: "Maximize both reward AND entropy of the policy, automatically balancing exploration and exploitation.", methodIntuition: "Don't just find a good strategy — find one that keeps your options open. SAC prefers diverse good actions over committing to a single action.", results: "SOTA on MuJoCo benchmarks with much better sample efficiency than PPO for continuous control.", strengths: "Automatic temperature tuning, stable off-policy training, sample efficient.", limitations: "Continuous actions only; entropy bonus can hinder precision in some tasks.", whoShouldRead: ["researcher"], tldr: "Maximize reward + entropy. Explore systematically, converge reliably. The go-to for continuous control.", keyInsight: "The entropy bonus prevents premature convergence — the policy stays stochastic enough to discover better solutions even late in training.", link: "https://arxiv.org/abs/1801.01290" }),
      makePaper({ id: "rl5", title: "RLHF: Training Language Models to Follow Instructions", year: 2022, difficulty: "Intermediate", authors: "Ouyang, Wu, Jiang et al.", institution: "OpenAI", citations: 7000, section: "modern", whyItMatters: "RLHF is what made ChatGPT helpful and safe. The technique that bridged raw language models and useful AI assistants.", problem: "Large LMs generate fluent text but don't follow instructions or human preferences.", coreContribution: "Three-stage process: supervised fine-tuning → reward model from human comparisons → PPO optimization against the reward model.", methodIntuition: "First teach the model to follow instructions (SFT). Then train a judge to score outputs by human preference. Finally, optimize the model to please the judge.", results: "InstructGPT (1.3B) was preferred over GPT-3 (175B) by human raters. Dramatic improvement in helpfulness and safety.", strengths: "Aligns models with human intent, reduces harmful outputs, scalable.", limitations: "Reward model can be gamed; human preferences are noisy and subjective.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "Human feedback → reward model → RL fine-tuning. The recipe that made ChatGPT actually useful.", keyInsight: "The reward model is the crucial bottleneck — it must capture nuanced human preferences, not just surface-level quality signals.", link: "https://arxiv.org/abs/2203.02155" }),
      makePaper({ id: "rl6", title: "Multi-Agent RL: Cooperation and Competition", year: 2017, difficulty: "Advanced", authors: "Lowe, Wu, Tamar et al.", institution: "OpenAI / UC Berkeley", citations: 5500, section: "modern", whyItMatters: "Extended RL to multi-agent settings where agents must cooperate or compete, essential for real-world applications.", problem: "Single-agent RL doesn't handle environments with multiple interacting agents. The environment becomes non-stationary from each agent's perspective.", coreContribution: "MADDPG: centralized training with decentralized execution — agents share information during training but act independently at test time.", methodIntuition: "During practice, the team watches game film together (centralized critic). During the game, each player makes their own decisions (decentralized actors).", results: "Solved cooperative and competitive multi-agent tasks that single-agent methods couldn't handle.", strengths: "Handles mixed cooperative-competitive settings, stable training.", limitations: "Scales poorly with number of agents; assumes access to other agents' actions during training.", whoShouldRead: ["researcher"], tldr: "Train together, act independently. Centralized critics + decentralized actors solve multi-agent challenges.", keyInsight: "The key insight is asymmetry: use extra information during training (others' actions) that won't be available at test time.", link: "https://arxiv.org/abs/1706.02275" }),
      makePaper({ id: "rl7", title: "Model-Based RL: World Models and Dreamer", year: 2020, difficulty: "Advanced", authors: "Hafner, Lillicrap, Norouzi et al.", institution: "Google Brain / DeepMind", citations: 3500, section: "modern", whyItMatters: "Learning a world model enables 'imagination' — the agent can plan by simulating future trajectories without real environment interaction.", problem: "Model-free RL requires millions of real environment interactions. Can we learn a model and plan in imagination?", coreContribution: "Learn a latent dynamics model, then train a policy entirely within the learned model's imagination.", methodIntuition: "Instead of practicing tennis by playing 10,000 real matches, build a mental model of physics and practice in your mind. Dreamer trains in 'dreams'.", results: "Achieved comparable performance to model-free methods with 50x fewer environment interactions on Atari and continuous control.", strengths: "Dramatically sample efficient, enables planning, transferable world models.", limitations: "Model errors compound over long horizons; struggles with complex visual environments.", whoShouldRead: ["researcher"], tldr: "Learn a world model, train in imagination. 50x more sample efficient. The future of practical RL.", keyInsight: "The latent dynamics model compresses observations into compact states where prediction is easier — planning happens in this learned abstract space, not pixel space.", link: "https://arxiv.org/abs/1912.01603" }),
      makePaper({ id: "rl8", title: "Reward Shaping and Curriculum Learning in RL", year: 2019, difficulty: "Beginner", authors: "Narvekar, Peng, Leonetti et al.", institution: "Various", citations: 2000, section: "improvement", whyItMatters: "Sparse rewards make RL nearly impossible. Reward shaping and curriculum design are essential practical techniques.", problem: "In most real tasks, reward is sparse (only at goal). The agent wanders randomly for millions of steps before accidentally succeeding.", coreContribution: "Systematic approaches to designing intermediate rewards and ordering tasks from easy to hard to accelerate learning.", methodIntuition: "Teaching a child to swim: start in shallow water with floaties (easy tasks, dense rewards), gradually move to deeper water (harder tasks, sparser rewards).", results: "Orders of magnitude speedup in training across robotics, game playing, and navigation tasks.", strengths: "Makes hard problems tractable, intuitive design principles, widely applicable.", limitations: "Reward shaping can introduce bias; curriculum design requires domain knowledge.", whoShouldRead: ["beginner", "interview prep"], tldr: "Shape rewards to guide learning, order tasks easy-to-hard. Makes impossible RL problems solvable.", keyInsight: "Potential-based reward shaping is the only form guaranteed to preserve optimal policies — other shaping can create unintended shortcuts.", link: "https://jmlr.org/papers/v21/19-483.html" }),
    ],
  },
  {
    keywords: ["random forest", "forest", "bagging", "ensemble", "decision tree", "tree", "boosting", "xgboost", "gradient boosting"],
    papers: [
      makePaper({ id: "rf1", title: "Random Forests", year: 2001, difficulty: "Beginner", authors: "Leo Breiman", institution: "UC Berkeley", citations: 95000, section: "foundation", whyItMatters: "Random Forests became one of the most widely used ML algorithms, providing strong out-of-the-box performance.", problem: "Single decision trees overfit easily. How can we build a powerful yet robust ensemble?", coreContribution: "Build many trees on random data and feature subsets, aggregate predictions via voting/averaging.", methodIntuition: "Ask 100 people for directions, each taking a different route to think. The average answer is remarkably accurate. Wisdom of crowds for decision trees.", results: "Competitive accuracy across hundreds of benchmarks with almost no tuning.", strengths: "Minimal hyperparameter tuning, handles mixed feature types, provides feature importance.", limitations: "Can be slow for very large datasets; less interpretable than single trees.", whoShouldRead: ["beginner", "interview prep"], tldr: "Build many trees on random subsets, let them vote. Simple, powerful, hard to beat.", keyInsight: "Feature randomness at each split decorrelates tree errors, making the ensemble far stronger than individuals.", link: "https://link.springer.com/article/10.1023/A:1010933404324" }),
      makePaper({ id: "rf2", title: "Bagging Predictors", year: 1996, difficulty: "Beginner", authors: "Leo Breiman", institution: "UC Berkeley", citations: 30000, section: "foundation", whyItMatters: "Bagging is the foundational technique behind Random Forests.", problem: "Individual classifiers are unstable. How do we reduce variance without increasing bias?", coreContribution: "Bootstrap sampling + aggregation: train on resampled datasets, average predictions.", methodIntuition: "Take multiple measurements with slightly different conditions, average the results.", results: "Reduced prediction error 10-25% on unstable classifiers.", strengths: "Simple, reduces variance, embarrassingly parallel.", limitations: "Doesn't help with high-bias models; increases training time linearly.", whoShouldRead: ["beginner", "interview prep"], tldr: "Resample, retrain, average. A simple idea that consistently improves unstable learners.", keyInsight: "Bagging reduces variance without increasing bias because bootstrap diversity creates uncorrelated errors.", link: "https://link.springer.com/article/10.1007/BF00058655" }),
      makePaper({ id: "rf3", title: "XGBoost: A Scalable Tree Boosting System", year: 2016, difficulty: "Intermediate", authors: "Tianqi Chen, Carlos Guestrin", institution: "University of Washington", citations: 45000, section: "improvement", whyItMatters: "XGBoost dominated Kaggle competitions and became the industry standard for tabular data. It's the most practically important ML algorithm for structured data.", problem: "Gradient boosting was effective but slow and memory-hungry. How to scale it to large datasets efficiently?", coreContribution: "Efficient implementation of gradient boosting with regularization, sparsity-aware algorithms, and cache-aware block structure.", methodIntuition: "Build trees one at a time, each fixing the mistakes of previous ones. XGBoost does this with extreme engineering efficiency — like a factory optimized for speed.", results: "Won 17 out of 29 Kaggle competitions in 2015. Became the default for tabular ML in industry.", strengths: "Handles missing data, built-in regularization, extremely fast, parallelizable.", limitations: "Requires more tuning than random forests; sequential boosting harder to parallelize than bagging.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "Gradient boosting, engineered for speed and scale. The king of tabular ML and Kaggle competitions.", keyInsight: "The regularization term Ω(f) = γT + ½λ||w||² in the objective penalizes tree complexity, preventing the overfitting that plagues naive boosting.", link: "https://arxiv.org/abs/1603.02754" }),
      makePaper({ id: "rf4", title: "Feature Importance in Random Forests", year: 2007, difficulty: "Intermediate", authors: "Strobl, Boulesteix, Zeileis, Hothorn", institution: "LMU Munich", citations: 8000, section: "improvement", whyItMatters: "Understanding which features matter is often more valuable than prediction itself.", problem: "Complex models are black boxes. How do we measure each feature's contribution?", coreContribution: "Permutation importance and Gini importance — two methods to rank features by impact.", methodIntuition: "To test if an ingredient matters, randomly replace it and see if the dish gets worse.", results: "Standard practice in exploratory analysis and feature engineering across industries.", strengths: "Model-agnostic (permutation), accounts for interactions, intuitive.", limitations: "Permutation importance is slow; Gini importance biased toward high-cardinality features.", whoShouldRead: ["beginner", "researcher"], tldr: "Shuffle features, measure accuracy drop. Essential for ML interpretability.", keyInsight: "Permutation importance is model-agnostic and accounts for interactions, unlike Gini importance.", link: "https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-25" }),
      makePaper({ id: "rf5", title: "LightGBM: A Highly Efficient Gradient Boosting Framework", year: 2017, difficulty: "Intermediate", authors: "Ke, Meng, Finley et al.", institution: "Microsoft Research", citations: 20000, section: "improvement", whyItMatters: "LightGBM made gradient boosting even faster than XGBoost with novel sampling and binning strategies.", problem: "XGBoost is still slow on very large datasets. Can we speed up tree building without losing accuracy?", coreContribution: "Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB) for faster training.", methodIntuition: "Focus on the hard examples (large gradients) and bundle sparse features together — do less work where it matters less.", results: "20x faster than XGBoost on large datasets with comparable or better accuracy.", strengths: "Extremely fast, handles large datasets, categorical feature support.", limitations: "Can overfit on small datasets; histogram-based splitting loses some precision.", whoShouldRead: ["interview prep", "researcher"], tldr: "Faster than XGBoost via smart sampling and feature bundling. The speed champion of gradient boosting.", keyInsight: "GOSS keeps all large-gradient instances and randomly samples small-gradient ones — focusing compute on the examples the model finds hardest.", link: "https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-framework" }),
      makePaper({ id: "rf6", title: "AdaBoost and the Boosting Paradigm", year: 1997, difficulty: "Beginner", authors: "Yoav Freund, Robert Schapire", institution: "AT&T Labs", citations: 40000, section: "foundation", whyItMatters: "AdaBoost proved that combining weak learners can create a strong learner — the theoretical foundation for all boosting methods.", problem: "Can many 'slightly better than random' classifiers be combined into a highly accurate one?", coreContribution: "Iteratively train weak classifiers, upweighting misclassified examples each round.", methodIntuition: "A teacher who gives extra attention to struggling students. Each round, the hardest examples get more weight, forcing the next classifier to focus on what's still wrong.", results: "Proved the boosting hypothesis. Won the first Gödel Prize in machine learning theory.", strengths: "Strong theoretical guarantees, simple algorithm, adaptive to hard examples.", limitations: "Sensitive to noise and outliers (upweights noisy examples); sequential training.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "Focus on mistakes, combine weak classifiers into strong ones. The algorithm that proved boosting works.", keyInsight: "The exponential loss means AdaBoost aggressively attacks misclassified points — powerful but also its Achilles' heel with noisy data.", link: "https://www.sciencedirect.com/science/article/pii/S002200009791504X" }),
      makePaper({ id: "rf7", title: "CatBoost: Gradient Boosting with Categorical Features", year: 2018, difficulty: "Intermediate", authors: "Prokhorenkova, Gusev, Vorobev et al.", institution: "Yandex", citations: 8000, section: "modern", whyItMatters: "CatBoost handles categorical features natively without manual encoding — a major practical advantage for real-world data.", problem: "Most gradient boosting implementations require manual encoding of categorical features, which can introduce bias or lose information.", coreContribution: "Ordered target encoding and oblivious trees that handle categorical features without preprocessing.", methodIntuition: "Instead of converting categories to numbers (losing meaning), CatBoost uses the historical target statistics for each category — but carefully ordered to prevent data leakage.", results: "Competitive with XGBoost and LightGBM on all benchmarks, while winning on datasets with many categorical features.", strengths: "Native categorical support, robust to overfitting, minimal preprocessing.", limitations: "Slower training than LightGBM; oblivious trees may be suboptimal for some tasks.", whoShouldRead: ["interview prep", "researcher"], tldr: "Gradient boosting that handles categories natively. No more manual encoding. Especially strong on categorical-heavy data.", keyInsight: "Ordered target encoding uses a random permutation to compute target statistics — each example only sees targets from previous examples in the permutation, preventing leakage.", link: "https://arxiv.org/abs/1706.09516" }),
      makePaper({ id: "rf8", title: "Interpretable ML: SHAP Values for Tree Ensembles", year: 2020, difficulty: "Intermediate", authors: "Lundberg, Erion, Chen et al.", institution: "University of Washington", citations: 12000, section: "modern", whyItMatters: "SHAP provides theoretically grounded feature attributions based on game theory, making tree ensemble predictions interpretable.", problem: "Feature importance tells you what matters globally, but not WHY a specific prediction was made.", coreContribution: "TreeSHAP: an exact, polynomial-time algorithm to compute Shapley values for tree ensemble predictions.", methodIntuition: "For each prediction, fairly distribute 'credit' among features using cooperative game theory — each feature's contribution is its average marginal effect across all possible feature orderings.", results: "Became the standard for ML explainability. Used in healthcare, finance, and legal compliance.", strengths: "Theoretically grounded (Shapley values), local + global explanations, consistent.", limitations: "Can be computationally expensive for non-tree models; explanations can be misinterpreted.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "Game theory meets ML: fairly attribute each feature's contribution to individual predictions. The gold standard for explainability.", keyInsight: "SHAP is the ONLY attribution method that satisfies local accuracy, missingness, and consistency simultaneously — these properties make it uniquely reliable.", link: "https://arxiv.org/abs/1905.04610" }),
    ],
  },
  {
    keywords: ["cnn", "convolutional", "convolution", "image", "vision", "computer vision", "object detection"],
    papers: [
      makePaper({ id: "cv1", title: "LeNet: Gradient-Based Learning for Document Recognition", year: 1998, difficulty: "Beginner", authors: "LeCun, Bottou, Bengio, Haffner", institution: "AT&T Labs", citations: 35000, section: "foundation", whyItMatters: "The first successful CNN — proved learned features beat hand-designed ones.", problem: "Could a neural network automatically learn visual features from raw pixels?", coreContribution: "Convolutional architecture with alternating convolution and pooling layers, trained end-to-end.", methodIntuition: "Show the network thousands of examples, let it figure out edges, curves, and strokes on its own.", results: ">99% accuracy on digit recognition. Deployed for bank check processing.", strengths: "End-to-end learning, weight sharing reduces parameters.", limitations: "Limited to simple patterns; didn't scale to complex images until GPUs.", whoShouldRead: ["beginner"], tldr: "Convolution + pooling + backprop = automatic visual feature learning. The original CNN.", keyInsight: "Weight sharing means the same detector works everywhere in the image — translation invariance from architecture.", link: "http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" }),
      makePaper({ id: "cv2", title: "VGGNet: Very Deep Convolutional Networks", year: 2014, difficulty: "Intermediate", authors: "Simonyan, Zisserman", institution: "University of Oxford", citations: 80000, section: "foundation", whyItMatters: "Showed depth is critical — stacking small 3×3 filters beats fewer large filters.", problem: "How deep should a CNN be? What filter sizes work best?", coreContribution: "Uniform 3×3 convolutions stacked deep, establishing depth as a key design principle.", methodIntuition: "Two 3×3 filters see the same area as one 5×5 but with more non-linearity and fewer parameters.", results: "92.7% top-5 on ImageNet. VGG-16/19 became standard pretrained backbones.", strengths: "Simple uniform architecture, excellent transfer learning backbone.", limitations: "Very large model (138M params), slow training, no skip connections.", whoShouldRead: ["beginner", "interview prep"], tldr: "Stack many small 3×3 filters. Depth matters more than filter size.", keyInsight: "3×3 is the smallest filter capturing spatial patterns. Using only 3×3 makes architecture decisions trivial: just choose depth.", link: "https://arxiv.org/abs/1409.1556" }),
      makePaper({ id: "cv3", title: "YOLO: Unified Real-Time Object Detection", year: 2016, difficulty: "Intermediate", authors: "Redmon, Divvala, Girshick, Farhadi", institution: "University of Washington", citations: 35000, section: "improvement", whyItMatters: "Turned object detection from slow multi-stage pipeline into a single fast network.", problem: "Detection systems like R-CNN were accurate but slow. Could we detect in a single pass?", coreContribution: "Frame detection as single regression — one network predicts boxes and classes in one evaluation.", methodIntuition: "Instead of scanning region by region, glance at the entire image once and identify all objects simultaneously.", results: "45 FPS with competitive accuracy. Later versions (v2-v8) dominate real-time detection.", strengths: "Real-time speed, global context awareness, simple architecture.", limitations: "Struggles with small objects and overlapping instances.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "One look, all objects detected. Real-time speed, competitive accuracy. Revolutionized practical detection.", keyInsight: "Global context means fewer background mistakes — YOLO understands the whole scene.", link: "https://arxiv.org/abs/1506.02640" }),
      makePaper({ id: "cv4", title: "U-Net: Convolutional Networks for Biomedical Image Segmentation", year: 2015, difficulty: "Intermediate", authors: "Ronneberger, Fischer, Brox", institution: "University of Freiburg", citations: 55000, section: "improvement", whyItMatters: "U-Net became the standard for image segmentation, especially in medical imaging where labeled data is scarce.", problem: "Semantic segmentation requires dense pixel-level predictions. How to combine low-level spatial detail with high-level semantic features?", coreContribution: "Encoder-decoder architecture with skip connections that concatenate encoder features to decoder layers, preserving spatial detail.", methodIntuition: "Compress the image to understand WHAT is there (encoder), then expand back to understand WHERE it is (decoder). Skip connections carry spatial details from encoder to decoder.", results: "Won ISBI cell tracking challenge with very few training images. Became the default for medical image segmentation.", strengths: "Works with very few training images, excellent spatial precision, simple architecture.", limitations: "Memory-intensive due to skip connections; fixed receptive field limits context.", whoShouldRead: ["researcher", "interview prep"], tldr: "Encoder-decoder with skip connections for pixel-level segmentation. The gold standard in medical imaging.", keyInsight: "Skip connections solve the detail-vs-context tradeoff: the decoder gets both semantic meaning (from deep layers) and spatial precision (from early layers).", link: "https://arxiv.org/abs/1505.04597" }),
      makePaper({ id: "cv5", title: "Feature Pyramid Networks for Object Detection", year: 2017, difficulty: "Advanced", authors: "Lin, Dollár, Girshick et al.", institution: "Facebook AI Research", citations: 15000, section: "improvement", whyItMatters: "FPN solved multi-scale detection by building a top-down feature pyramid from any CNN backbone.", problem: "Objects appear at different scales in images. How to detect both tiny and large objects effectively?", coreContribution: "Top-down pathway with lateral connections that creates rich multi-scale feature maps from a single forward pass.", methodIntuition: "Look at the image at multiple zoom levels simultaneously — fine details at high resolution, big patterns at low resolution — then combine them.", results: "Significant improvement in detecting small objects. Became a standard component in all modern detectors.", strengths: "Multi-scale features from single pass, minimal extra computation, backbone-agnostic.", limitations: "Adds architectural complexity; top-down fusion can lose fine-grained details.", whoShouldRead: ["researcher"], tldr: "Build a multi-scale feature pyramid from any backbone. Detect objects at all sizes.", keyInsight: "High-level features have strong semantics but poor resolution; low-level features have weak semantics but precise localization. FPN combines both.", link: "https://arxiv.org/abs/1612.03144" }),
      makePaper({ id: "cv6", title: "Vision Transformer (ViT): An Image is Worth 16x16 Words", year: 2021, difficulty: "Advanced", authors: "Dosovitskiy, Beyer, Kolesnikov et al.", institution: "Google Brain", citations: 25000, section: "modern", whyItMatters: "Proved that pure transformers can match or beat CNNs on vision tasks when given enough data.", problem: "Transformers dominate NLP. Can they replace CNNs for vision without convolutions?", coreContribution: "Split images into 16×16 patches, treat each as a token, apply standard transformer encoder.", methodIntuition: "Cut an image into a grid of tiles. Feed each tile into a transformer as if it were a word. Let attention figure out spatial relationships.", results: "Matched SOTA CNNs on ImageNet with large-scale pre-training. Sparked a revolution in computer vision.", strengths: "Scales with data and compute, global attention from first layer, unified architecture with NLP.", limitations: "Requires large datasets (worse than CNNs on small data); lacks inductive bias for spatial structure.", whoShouldRead: ["researcher", "interview prep"], tldr: "Treat image patches as tokens, apply transformer. Convolutions not required. Vision's transformer moment.", keyInsight: "Without CNN's spatial inductive bias, ViT needs more data — but given enough data, learned attention patterns outperform hardcoded convolution.", link: "https://arxiv.org/abs/2010.11929" }),
      makePaper({ id: "cv7", title: "GAN-based Image Super-Resolution (ESRGAN)", year: 2018, difficulty: "Advanced", authors: "Wang, Yu, Wu et al.", institution: "Chinese University of HK", citations: 8000, section: "modern", whyItMatters: "Made AI image upscaling photorealistic, transforming industries from gaming to satellite imagery.", problem: "Standard upscaling produces blurry results. Can we generate realistic high-frequency details?", coreContribution: "Residual-in-Residual Dense Blocks with relativistic GAN loss for perceptually realistic super-resolution.", methodIntuition: "Instead of just making pixels bigger, imagine what the fine details SHOULD look like based on context. The GAN ensures generated details look real.", results: "4x upscaling with photorealistic quality. Won PIRM 2018 challenge. Widely deployed in production.", strengths: "Perceptually superior to MSE-based methods, generates convincing textures.", limitations: "Can hallucinate incorrect details; slower than simple upscaling.", whoShouldRead: ["researcher"], tldr: "AI upscaling that generates realistic details. Not just bigger pixels — actually better resolution.", keyInsight: "Perceptual loss (feature-space distance) produces sharper results than pixel-space MSE, which optimizes for PSNR but produces blurry averages.", link: "https://arxiv.org/abs/1809.00219" }),
      makePaper({ id: "cv8", title: "Segment Anything Model (SAM)", year: 2023, difficulty: "Advanced", authors: "Kirillov, Mintun, Ravi et al.", institution: "Meta AI", citations: 5000, section: "modern", whyItMatters: "Foundation model for segmentation — segment any object in any image with zero-shot generalization.", problem: "Image segmentation models need task-specific training. Can we build one model that segments everything?", coreContribution: "A promptable segmentation model trained on 11M images and 1.1B masks, generalizing to any segmentation task.", methodIntuition: "Like GPT for text but for images — a general-purpose segmentation engine you can prompt with points, boxes, or text.", results: "Zero-shot transfer to dozens of segmentation benchmarks competitive with specialized models.", strengths: "Zero-shot generalization, promptable interface, massive training data.", limitations: "Requires prompts (not fully automatic); struggles with fine-grained boundaries.", whoShouldRead: ["researcher"], tldr: "One model to segment them all. Point, click, segment. The foundation model for image segmentation.", keyInsight: "The key is the data engine: model-assisted annotation creates more data, which trains a better model, in a self-improving loop.", link: "https://arxiv.org/abs/2304.02643" }),
    ],
  },
  {
    keywords: ["gan", "generative adversarial", "generative model", "image generation", "synthesis"],
    papers: [
      makePaper({ id: "gan1", title: "Generative Adversarial Networks", year: 2014, difficulty: "Intermediate", authors: "Ian Goodfellow et al.", institution: "Université de Montréal", citations: 65000, section: "foundation", whyItMatters: "Introduced adversarial training — two networks competing to generate realistic data.", problem: "Previous generative models produced blurry outputs. Could competition produce sharper results?", coreContribution: "Generator vs discriminator minimax game where competition drives realistic generation.", methodIntuition: "A counterfeiter and detective competing. As detection improves, counterfeiting must too.", results: "First sharp synthetic images. Spawned thousands of variants.", strengths: "Sharp outputs, versatile framework, no explicit density estimation.", limitations: "Training instability, mode collapse, no likelihood estimation.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "Two networks compete: generate and detect fakes. Competition produces stunning results.", keyInsight: "The generator never sees real data — it learns only from the discriminator's feedback.", link: "https://arxiv.org/abs/1406.2661" }),
      makePaper({ id: "gan2", title: "StyleGAN: A Style-Based Generator Architecture", year: 2019, difficulty: "Advanced", authors: "Karras, Laine, Aila", institution: "NVIDIA", citations: 12000, section: "improvement", whyItMatters: "Produced the most photorealistic synthetic faces ever seen.", problem: "Standard GANs had limited control over generated images. How to separately control different attributes?", coreContribution: "Style injection at each resolution level for hierarchical control of image attributes.", methodIntuition: "Paint at different scales: first sketch composition, then details, then textures.", results: "1024×1024 faces that fooled human observers. 'This Person Does Not Exist' went viral.", strengths: "Unprecedented control, disentangled style space, photorealistic quality.", limitations: "Expensive training, limited to aligned datasets (faces, cars).", whoShouldRead: ["researcher"], tldr: "Control image generation at every scale. Most realistic synthetic faces ever.", keyInsight: "The mapping network creates disentangled W space — changing one attribute doesn't affect others.", link: "https://arxiv.org/abs/1812.04948" }),
      makePaper({ id: "gan3", title: "CycleGAN: Unpaired Image-to-Image Translation", year: 2017, difficulty: "Intermediate", authors: "Zhu, Park, Isola, Efros", institution: "UC Berkeley", citations: 18000, section: "improvement", whyItMatters: "Image translation without paired data — horses to zebras, summer to winter.", problem: "Paired training data is rare. Can we learn translation from unpaired collections?", coreContribution: "Cycle consistency: translate A→B→A and recover the original.", methodIntuition: "Translate English→French→English — you should get your original sentence back.", results: "Dozens of domain translations. Widely adopted in art and medical imaging.", strengths: "No paired data needed, versatile, intuitive constraint.", limitations: "Can produce artifacts, struggles with large shape changes.", whoShouldRead: ["beginner", "researcher"], tldr: "Translate between styles without paired examples. Round-trip consistency makes it work.", keyInsight: "Cycle consistency prevents mode collapse — the generator must produce reversible transformations.", link: "https://arxiv.org/abs/1703.10593" }),
      makePaper({ id: "gan4", title: "Pix2Pix: Image-to-Image Translation with Conditional GANs", year: 2017, difficulty: "Intermediate", authors: "Isola, Zhu, Zhou, Efros", institution: "UC Berkeley", citations: 16000, section: "foundation", whyItMatters: "General framework for paired image translation — sketches to photos, day to night, labels to facades.", problem: "Many vision problems are image→image translations. Can one framework handle all of them?", coreContribution: "Conditional GAN with U-Net generator and PatchGAN discriminator for general paired image translation.", methodIntuition: "Show the network pairs of images (input→output). It learns the transformation. Like showing an artist many example transformations.", results: "Impressive results across dozens of tasks from a single architecture.", strengths: "General-purpose, high-quality results, simple framework.", limitations: "Requires paired training data; struggles with large structural changes.", whoShouldRead: ["beginner", "researcher"], tldr: "One framework for all paired image translation. Sketches to photos, labels to scenes.", keyInsight: "PatchGAN discriminator focuses on local texture rather than global structure — it checks if each patch looks real, not the whole image.", link: "https://arxiv.org/abs/1611.07004" }),
      makePaper({ id: "gan5", title: "Progressive GAN: Training GANs Layer by Layer", year: 2018, difficulty: "Advanced", authors: "Karras, Aila, Laine, Lehtinen", institution: "NVIDIA", citations: 8000, section: "improvement", whyItMatters: "Enabled high-resolution GAN training by growing the network progressively from 4×4 to 1024×1024.", problem: "Training GANs directly at high resolution is unstable. How to generate 1024×1024 images?", coreContribution: "Start training at low resolution, progressively add layers. Each resolution stabilizes before adding the next.", methodIntuition: "A painter who first sketches the rough outline, then gradually adds finer and finer details. Each stage builds on the previous.", results: "First GANs to generate convincing 1024×1024 faces. Pioneered high-res generation.", strengths: "Stable high-res training, gradually increasing complexity, smooth transitions.", limitations: "Long training times, complex training schedule.", whoShouldRead: ["researcher"], tldr: "Grow the GAN from low to high resolution. Stable training, stunning 1024×1024 results.", keyInsight: "Progressive training means the generator never has to learn the full distribution at once — it masters coarse structure before fine details.", link: "https://arxiv.org/abs/1710.10196" }),
      makePaper({ id: "gan6", title: "Wasserstein GAN: Improved Training of Generative Models", year: 2017, difficulty: "Advanced", authors: "Arjovsky, Chintala, Bottou", institution: "Facebook AI / Courant Institute", citations: 15000, section: "improvement", whyItMatters: "Fixed GAN training instability by replacing JS divergence with Wasserstein distance.", problem: "Standard GAN loss is uninformative and training is unstable with mode collapse. Can we find a better loss?", coreContribution: "Earth Mover (Wasserstein) distance provides meaningful gradient signals even when distributions don't overlap.", methodIntuition: "Instead of asking 'can you tell fake from real?' (binary), ask 'how much work to transform fake distribution into real one?' This always gives useful gradients.", results: "More stable training, meaningful loss curves that correlate with sample quality.", strengths: "Stable gradients, meaningful loss metric, theoretical elegance.", limitations: "Weight clipping is crude (improved by gradient penalty in WGAN-GP).", whoShouldRead: ["researcher", "interview prep"], tldr: "Replace JS divergence with Wasserstein distance. GAN training becomes stable and loss becomes meaningful.", keyInsight: "When distributions don't overlap (common early in training), JS divergence is constant — the generator gets zero gradient. Wasserstein distance always provides useful gradients.", link: "https://arxiv.org/abs/1701.07875" }),
      makePaper({ id: "gan7", title: "Conditional Image Generation with Class-Conditional GANs (BigGAN)", year: 2019, difficulty: "Advanced", authors: "Brock, Donahue, Simonyan", institution: "DeepMind", citations: 6000, section: "modern", whyItMatters: "Showed that GANs benefit enormously from scale — bigger batches and wider networks produce dramatically better images.", problem: "GAN quality plateaued at moderate resolution. Does scaling up help, and how to make it stable?", coreContribution: "Scaled up GANs with large batch sizes, spectral normalization, and truncation trick for quality-diversity tradeoff.", methodIntuition: "Like cooking: better ingredients (more compute, bigger batches) and better technique (spectral norm) produce better dishes.", results: "Unprecedented class-conditional image quality on ImageNet at 256×256 and 512×512.", strengths: "Best class-conditional quality, clean training, useful truncation control.", limitations: "Enormous compute requirements; truncation trades diversity for quality.", whoShouldRead: ["researcher"], tldr: "Scale up everything: batch size, model width, compute. GANs benefit enormously from scale.", keyInsight: "The truncation trick: sampling from a truncated normal (|z|<threshold) trades diversity for quality — this single knob controls the fidelity-diversity spectrum.", link: "https://arxiv.org/abs/1809.11096" }),
      makePaper({ id: "gan8", title: "Neural Radiance Fields (NeRF) for View Synthesis", year: 2020, difficulty: "Advanced", authors: "Mildenhall, Srinivasan, Tancik et al.", institution: "UC Berkeley / Google", citations: 12000, section: "modern", whyItMatters: "NeRF synthesizes photorealistic novel views of scenes from sparse photographs — transforming 3D content creation.", problem: "How to render new viewpoints of a scene given only a few photographs?", coreContribution: "Represent a scene as a continuous volumetric function (position → color + density) using a neural network, trained from multiple views.", methodIntuition: "Teach a neural network to be a 3D scene. Given any point in space and viewing direction, it tells you the color and opacity. Then render by shooting rays through the scene.", results: "Stunning novel view synthesis from ~20-50 input images. Sparked explosion of neural rendering research.", strengths: "Photorealistic quality, continuous representation, works from sparse views.", limitations: "Slow training and rendering; per-scene optimization required.", whoShouldRead: ["researcher"], tldr: "A neural network IS the 3D scene. Feed it coordinates, get color. Render any viewpoint.", keyInsight: "Positional encoding with high frequencies enables the MLP to represent fine details — without it, the network produces blurry, low-frequency outputs.", link: "https://arxiv.org/abs/2003.08934" }),
    ],
  },
  {
    keywords: ["clustering", "k-means", "kmeans", "dbscan", "unsupervised", "hierarchical clustering"],
    papers: [
      makePaper({ id: "cl1", title: "Lloyd's Algorithm: K-Means Clustering", year: 1957, difficulty: "Beginner", authors: "Stuart Lloyd", institution: "Bell Labs", citations: 55000, section: "foundation", whyItMatters: "The most widely used clustering algorithm — foundational for unsupervised learning.", problem: "How to automatically discover groups in unlabeled data?", coreContribution: "Iterative assignment-update: assign points to nearest centroid, move centroids to cluster means.", methodIntuition: "Drop K pins on a map. Each point joins the nearest pin's group. Move pins to group centers. Repeat.", results: "Foundational algorithm used in virtually every domain.", strengths: "Simple, fast, scalable, interpretable.", limitations: "Requires specifying K, sensitive to initialization, assumes spherical clusters.", whoShouldRead: ["beginner", "interview prep"], tldr: "Assign, update, repeat. The simplest clustering that works.", keyInsight: "Each iteration decreases within-cluster sum of squares, guaranteeing convergence.", link: "https://cs.nyu.edu/~roweis/csc2515-2006/readings/lloyd.pdf" }),
      makePaper({ id: "cl2", title: "DBSCAN: Density-Based Clustering with Noise", year: 1996, difficulty: "Intermediate", authors: "Ester, Kriegel, Sander, Xu", institution: "University of Munich", citations: 30000, section: "foundation", whyItMatters: "Handles arbitrary cluster shapes and automatically detects outliers.", problem: "K-Means assumes spherical clusters and requires knowing K. How to handle arbitrary shapes and noise?", coreContribution: "Density-based clustering: dense regions are clusters, sparse regions are boundaries.", methodIntuition: "Walk through a crowd. Dense areas are clusters, isolated people are noise.", results: "Standard for spatial analysis, anomaly detection, and irregular cluster shapes.", strengths: "No K required, arbitrary shapes, noise detection.", limitations: "Struggles with varying densities; sensitive to ε and MinPts.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "Find dense regions, call them clusters, ignore noise. No K needed.", keyInsight: "Core/border/noise classification is elegant: core points define interiors, border points touch clusters, everything else is noise.", link: "https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf" }),
      makePaper({ id: "cl3", title: "Elbow Method and Silhouette Analysis", year: 2004, difficulty: "Beginner", authors: "Thorndike / Rousseeuw", institution: "Various", citations: 12000, section: "foundation", whyItMatters: "Principled ways to choose K — the biggest practical challenge in clustering.", problem: "K-Means needs K specified. How to choose when we don't know?", coreContribution: "Elbow method (plot inertia vs K) + silhouette scores (measure cluster quality per point).", methodIntuition: "Adding clusters always reduces error, but improvement becomes marginal — find that bend. Silhouette measures if points are in the right neighborhood.", results: "Standard practice in applied clustering across industries.", strengths: "Simple, intuitive, visual.", limitations: "Elbow can be ambiguous; silhouette is O(n²).", whoShouldRead: ["beginner", "interview prep"], tldr: "Plot error vs K, find the elbow. Compute silhouette for validation.", keyInsight: "s(i) = (b-a)/max(a,b): values near 1 mean perfect clustering.", link: "https://www.sciencedirect.com/science/article/pii/0377042787901257" }),
      makePaper({ id: "cl4", title: "K-Means++: Careful Seeding for Better Initialization", year: 2007, difficulty: "Beginner", authors: "Arthur, Vassilvitskii", institution: "Stanford", citations: 10000, section: "improvement", whyItMatters: "Smart initialization that makes K-Means consistently better and provably O(log k)-competitive.", problem: "Random initialization often leads to poor local optima. How to start K-Means well?", coreContribution: "Select initial centroids proportional to squared distance from existing centroids.", methodIntuition: "Don't drop all pins in the same corner. Spread them out: each new pin goes where existing pins are far away.", results: "Consistently 2-10x better clustering quality than random init. Now the default in all implementations.", strengths: "Simple, provable guarantees, minimal overhead, drop-in replacement.", limitations: "Still O(nkd) per iteration; sequential initialization can be slow for very large k.", whoShouldRead: ["beginner", "interview prep"], tldr: "Smart initialization for K-Means: spread centroids out. Better results, same algorithm.", keyInsight: "Sampling proportional to D(x)² ensures centroids are well-separated — this single change provides O(log k) approximation guarantees.", link: "https://theory.stanford.edu/~sergei/papers/kMeansPP-soda.pdf" }),
      makePaper({ id: "cl5", title: "Gaussian Mixture Models and EM Algorithm", year: 1977, difficulty: "Intermediate", authors: "Dempster, Laird, Rubin", institution: "Harvard", citations: 80000, section: "foundation", whyItMatters: "EM is one of the most important algorithms in statistics. GMMs extend K-Means to soft probabilistic clustering.", problem: "K-Means assigns each point to exactly one cluster. What if cluster membership is uncertain or overlapping?", coreContribution: "Expectation-Maximization: alternate between computing soft assignments (E-step) and updating parameters (M-step).", methodIntuition: "K-Means says 'this point IS in cluster 2.' GMMs say 'this point is 70% cluster 2, 25% cluster 1, 5% cluster 3.' Soft assignments capture uncertainty.", results: "Foundation for probabilistic modeling. Used in speech recognition, document clustering, anomaly detection.", strengths: "Soft assignments, handles overlapping clusters, principled probabilistic framework.", limitations: "Sensitive to initialization; EM finds local optima; covariance estimation fails in high dimensions.", whoShouldRead: ["researcher", "interview prep"], tldr: "Probabilistic K-Means: soft cluster assignments via EM. The principled way to cluster when uncertainty matters.", keyInsight: "The E-step computes responsibilities (how much each cluster 'owns' each point). The M-step updates cluster parameters to match. This converges to a local maximum of likelihood.", link: "https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1977.tb01600.x" }),
      makePaper({ id: "cl6", title: "Hierarchical Clustering: Agglomerative and Divisive", year: 1963, difficulty: "Beginner", authors: "Ward, Johnson", institution: "Various", citations: 20000, section: "foundation", whyItMatters: "Produces a full tree of cluster relationships (dendrogram), revealing structure at all granularities.", problem: "How to discover cluster structure without specifying K? Can we see how clusters relate to each other?", coreContribution: "Build a tree by iteratively merging (agglomerative) or splitting (divisive) clusters based on distance criteria.", methodIntuition: "Start with each point alone. Merge the two closest groups. Repeat until everything is one group. The resulting tree shows the natural cluster hierarchy.", results: "Standard in biology (phylogenetics), social sciences, and exploratory data analysis.", strengths: "No K needed, full hierarchy, dendrogram visualization.", limitations: "O(n³) time, cannot undo merges, sensitive to linkage criterion.", whoShouldRead: ["beginner"], tldr: "Build a tree of merges from individual points to one cluster. See structure at every level.", keyInsight: "Linkage criterion (single, complete, average, Ward's) dramatically affects results — Ward's method minimizes variance increase, usually producing the most balanced clusters.", link: "https://doi.org/10.1080/01621459.1963.10500845" }),
      makePaper({ id: "cl7", title: "Spectral Clustering via Graph Laplacians", year: 2007, difficulty: "Advanced", authors: "Von Luxburg", institution: "Max Planck Institute", citations: 15000, section: "improvement", whyItMatters: "Handles complex cluster shapes by clustering in the eigenspace of the similarity graph Laplacian.", problem: "K-Means and GMMs assume convex clusters. How to cluster arbitrarily shaped groups?", coreContribution: "Build a similarity graph, compute eigenvectors of the Laplacian, cluster in this new space.", methodIntuition: "Build a network connecting similar points. Find the graph's natural 'vibration modes' (eigenvectors). Points that vibrate together belong together.", results: "Excellent on non-convex clusters where K-Means fails. Standard for image segmentation and community detection.", strengths: "Handles any cluster shape, elegant theory, graph-based.", limitations: "O(n³) for eigendecomposition; requires choosing number of clusters and similarity parameters.", whoShouldRead: ["researcher", "interview prep"], tldr: "Cluster in eigenspace of the graph Laplacian. Handles any shape K-Means can't.", keyInsight: "The number of zero eigenvalues of the Laplacian equals the number of connected components — the eigengap tells you the natural number of clusters.", link: "https://link.springer.com/article/10.1007/s11222-007-9033-z" }),
      makePaper({ id: "cl8", title: "t-SNE: Visualizing High-Dimensional Data", year: 2008, difficulty: "Intermediate", authors: "Van der Maaten, Hinton", institution: "Tilburg University / University of Toronto", citations: 30000, section: "modern", whyItMatters: "The most popular technique for visualizing high-dimensional data in 2D. Reveals cluster structure invisible to PCA.", problem: "High-dimensional data is impossible to visualize. How to reduce to 2D while preserving local structure?", coreContribution: "Minimize KL divergence between probability distributions over pairs in high-D and low-D space.", methodIntuition: "Points that are neighbors in high dimensions should be neighbors in the 2D plot. t-SNE finds a 2D arrangement that best preserves these neighborhoods.", results: "Became the standard visualization tool. Produces beautiful cluster visualizations of embeddings, gene expression, etc.", strengths: "Excellent local structure preservation, reveals clusters, beautiful visualizations.", limitations: "Non-parametric (can't project new points), perplexity-sensitive, distances between clusters not meaningful.", whoShouldRead: ["beginner", "researcher"], tldr: "Project high-dimensional data to 2D preserving neighborhoods. The visualization tool everyone uses.", keyInsight: "The 't' in t-SNE uses a Student-t distribution in low-D space — its heavier tails prevent the 'crowding problem' where moderate distances are crushed together.", link: "https://jmlr.org/papers/v9/vandermaaten08a.html" }),
    ],
  },
  {
    keywords: ["svm", "support vector", "kernel", "margin", "hyperplane"],
    papers: [
      makePaper({ id: "svm1", title: "A Tutorial on Support Vector Machines", year: 1998, difficulty: "Beginner", authors: "Burges", institution: "Bell Labs", citations: 25000, section: "foundation", whyItMatters: "SVMs dominated ML for over a decade. Understanding them is fundamental.", problem: "How to find the best decision boundary that will generalize best?", coreContribution: "Maximum margin classification — widest possible separator between classes.", methodIntuition: "Draw the widest road between two groups. The wider the margin, the more confident the classification.", results: "SOTA on handwriting, text classification, bioinformatics through the 2000s.", strengths: "Strong theoretical guarantees, works in high dimensions, sparse solution.", limitations: "Doesn't scale well to very large datasets; kernel choice requires expertise.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "Find the widest margin between classes. Only support vectors matter.", keyInsight: "Only support vectors define the boundary — remove all other points and get the same classifier.", link: "https://link.springer.com/article/10.1023/A:1009715923555" }),
      makePaper({ id: "svm2", title: "The Kernel Trick for Non-Linear Classification", year: 1999, difficulty: "Intermediate", authors: "Scholkopf, Smola", institution: "Max Planck Institute", citations: 18000, section: "foundation", whyItMatters: "One of the most elegant ideas in ML — solve non-linear problems without high-dimensional computation.", problem: "SVMs find linear boundaries, but data is often non-linear. How to classify without expensive feature mapping?", coreContribution: "Kernel functions compute inner products in high-D space without explicit transformation.", methodIntuition: "Lift mixed points into 3D where they become separable — the kernel does this lifting implicitly.", results: "Enabled non-linear classification with RBF, polynomial, and custom kernels.", strengths: "Elegant theory, infinite-dimensional features tractable, flexible.", limitations: "Kernel selection is an art; O(n²) memory for kernel matrix.", whoShouldRead: ["researcher", "interview prep"], tldr: "Compute in infinite dimensions without going there. Beautiful math, practical power.", keyInsight: "K(x,y) = φ(x)·φ(y) — for RBF kernel, φ maps to infinite dimensions but K is trivial to compute.", link: "https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf" }),
      makePaper({ id: "svm3", title: "Soft Margin SVMs and Regularization", year: 1995, difficulty: "Intermediate", authors: "Cortes, Vapnik", institution: "AT&T Bell Labs", citations: 50000, section: "foundation", whyItMatters: "Made SVMs practical for noisy real-world data by allowing controlled misclassification.", problem: "Hard-margin SVMs fail on non-separable data. How to handle noise gracefully?", coreContribution: "Slack variables and C parameter controlling margin vs. error tradeoff.", methodIntuition: "Allow some people on the wrong side of the wall, but charge a penalty. C controls strictness.", results: "Standard implementation in sklearn, LIBSVM, every major ML library.", strengths: "Handles noise, C provides clear bias-variance control.", limitations: "C requires tuning; quadratic programming is expensive for large datasets.", whoShouldRead: ["beginner", "interview prep"], tldr: "Allow mistakes but penalize them. C controls strictness. Made SVMs practical.", keyInsight: "Large C = low bias (fits closely), small C = low variance (wider margin). C is the bias-variance knob.", link: "https://link.springer.com/article/10.1007/BF00994018" }),
      makePaper({ id: "svm4", title: "One-Class SVM for Anomaly Detection", year: 2001, difficulty: "Intermediate", authors: "Scholkopf, Platt, Shawe-Taylor et al.", institution: "Microsoft Research", citations: 8000, section: "improvement", whyItMatters: "Extended SVMs to unsupervised anomaly detection — find the boundary around normal data.", problem: "We only have examples of 'normal' data. How to detect anomalies without negative examples?", coreContribution: "Find a hyperplane that separates data from the origin with maximum margin in feature space.", methodIntuition: "Draw the tightest possible boundary around normal data. Anything outside is an anomaly.", results: "Widely used in fraud detection, network intrusion detection, and manufacturing quality control.", strengths: "Works with only positive examples, kernel-flexible, principled.", limitations: "Sensitive to kernel/nu parameters; assumes anomalies are rare.", whoShouldRead: ["researcher", "interview prep"], tldr: "Draw a boundary around normal data. Detect anomalies as anything outside.", keyInsight: "The parameter ν controls the fraction of data treated as outliers — it's both an upper bound on the fraction of anomalies and a lower bound on support vectors.", link: "https://papers.nips.cc/paper/1999/hash/8725fb777f25776ffa9076e44fcfd776" }),
      makePaper({ id: "svm5", title: "SVM vs Deep Learning: When to Use What", year: 2019, difficulty: "Beginner", authors: "Survey compilation", institution: "Various", citations: 3000, section: "modern", whyItMatters: "Practical guidance on when SVMs still beat deep learning — essential for real-world ML decisions.", problem: "Deep learning dominates benchmarks, but is it always the best choice? When do classical methods win?", coreContribution: "Comprehensive comparison showing SVMs excel on small datasets, tabular data, and interpretability-critical applications.", methodIntuition: "A sports car is great on highways but terrible on rough terrain. SVMs are the reliable SUV — excellent when conditions aren't ideal for deep learning.", results: "SVMs outperform deep learning on datasets <10K samples and tabular data without spatial/temporal structure.", strengths: "Practical, evidence-based, helps avoid over-engineering.", limitations: "Guidelines may shift as deep learning advances.", whoShouldRead: ["beginner", "interview prep"], tldr: "SVMs win on small data and tabular problems. Deep learning wins on large data with structure. Know when to use each.", keyInsight: "The crossover point is roughly 10K-50K labeled examples — below that, SVMs often win because they have better inductive bias for the small-data regime.", link: "https://arxiv.org/search/?query=svm+deep+learning+comparison" }),
      makePaper({ id: "svm6", title: "Support Vector Regression (SVR)", year: 1997, difficulty: "Intermediate", authors: "Drucker, Burges, Kaufman et al.", institution: "AT&T Labs", citations: 15000, section: "improvement", whyItMatters: "Extended SVMs from classification to regression, using the ε-insensitive loss for robust fitting.", problem: "SVMs only handle classification. Can the maximum margin principle work for regression?", coreContribution: "ε-SVR: find a tube of width ε around the regression function that contains most training points.", methodIntuition: "Instead of drawing a line through points, draw a tube around the line. Points inside the tube contribute zero loss — only outliers are penalized.", results: "Excellent for robust regression especially with kernel methods. Widely used in financial forecasting.", strengths: "Robust to outliers (ε-insensitivity), kernel-flexible, sparse solution.", limitations: "ε and C require tuning; slower than OLS for simple problems.", whoShouldRead: ["researcher", "interview prep"], tldr: "SVMs for regression: fit a tube around the data. Robust, kernel-powered, sparse.", keyInsight: "The ε-insensitive zone means small residuals are completely ignored — the model focuses only on fitting the overall trend, not chasing noise.", link: "https://papers.nips.cc/paper/1996/hash/d38901788c533e8286cb6400b40b386d" }),
      makePaper({ id: "svm7", title: "Multi-Class SVMs: One-vs-All and One-vs-One", year: 1999, difficulty: "Beginner", authors: "Hsu, Lin", institution: "National Taiwan University", citations: 12000, section: "improvement", whyItMatters: "SVMs are inherently binary. Multi-class extensions are essential for practical classification.", problem: "SVMs separate two classes. How to handle problems with 3+ classes?", coreContribution: "Systematic comparison of OvA (K classifiers), OvO (K(K-1)/2 classifiers), and direct multi-class formulations.", methodIntuition: "OvA: train each class vs all others, pick the most confident. OvO: hold tournaments between every pair, pick the class that wins most.", results: "OvO with voting generally most accurate; OvA most efficient. Both are now standard.", strengths: "Simple, well-understood, works with any binary SVM.", limitations: "OvO scales quadratically with classes; class imbalance in OvA.", whoShouldRead: ["beginner", "interview prep"], tldr: "Binary SVMs → multi-class: either one-vs-all or round-robin tournaments. Both work well.", keyInsight: "OvO is preferred in practice because each sub-problem has balanced classes and fewer examples, making training faster despite more classifiers.", link: "https://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.pdf" }),
      makePaper({ id: "svm8", title: "Kernel PCA and Kernel Methods Beyond Classification", year: 1998, difficulty: "Advanced", authors: "Scholkopf, Smola, Muller", institution: "Max Planck Institute", citations: 10000, section: "modern", whyItMatters: "Extended the kernel trick beyond SVMs — kernel PCA, kernel regression, kernel clustering unlock non-linear versions of every linear method.", problem: "The kernel trick works beautifully for SVMs. Can we kernelize other algorithms too?", coreContribution: "A general framework for applying kernel methods to PCA, regression, and other algorithms via the kernel matrix.", methodIntuition: "If you can express an algorithm purely in terms of dot products, you can 'kernelize' it — replace dot products with kernel evaluations to go non-linear.", results: "Kernel PCA reveals non-linear structure that standard PCA misses. Became a general tool for non-linear dimensionality reduction.", strengths: "General framework, reveals non-linear structure, elegant theory.", limitations: "O(n²) kernel matrix storage; kernel selection remains challenging.", whoShouldRead: ["researcher"], tldr: "Any algorithm using dot products can be kernelized. Non-linear PCA, regression, and more.", keyInsight: "The 'kernel trick' is universal: if your algorithm only accesses data through inner products ⟨x,y⟩, replacing with K(x,y) gives you the non-linear version for free.", link: "https://link.springer.com/article/10.1007/s11222-005-4073-0" }),
    ],
  },
  {
    keywords: ["gradient descent", "optimization", "optimizer", "adam", "sgd", "learning rate"],
    papers: [
      makePaper({ id: "opt1", title: "Adam: A Method for Stochastic Optimization", year: 2014, difficulty: "Beginner", authors: "Kingma, Ba", institution: "University of Amsterdam / University of Toronto", citations: 160000, section: "foundation", whyItMatters: "The default optimizer for deep learning.", problem: "SGD needs careful learning rate tuning. Can we adapt per-parameter automatically?", coreContribution: "Momentum + RMSProp with bias correction = adaptive per-parameter learning rates.", methodIntuition: "Hiking in fog: remember direction (momentum) and terrain steepness (adaptive rates). Big steps on flat terrain, careful on steep slopes.", results: "Default optimizer in PyTorch, TensorFlow, everywhere.", strengths: "Works out of the box, fast convergence, per-parameter adaptation.", limitations: "May not generalize as well as SGD+momentum on some tasks.", whoShouldRead: ["beginner", "interview prep"], tldr: "Adaptive learning rates + momentum + bias correction = optimizer that just works.", keyInsight: "Bias correction in early steps is crucial — without it, moment estimates are biased toward zero.", link: "https://arxiv.org/abs/1412.6980" }),
      makePaper({ id: "opt2", title: "SGD with Nesterov Momentum", year: 2013, difficulty: "Intermediate", authors: "Sutskever, Martens, Dahl, Hinton", institution: "University of Toronto", citations: 12000, section: "foundation", whyItMatters: "Properly tuned SGD with momentum matches or beats adaptive methods.", problem: "Is SGD truly inferior to adaptive methods? How much do initialization and momentum matter?", coreContribution: "Nesterov momentum with proper initialization achieves competitive results.", methodIntuition: "A ball rolling downhill that 'looks ahead' — calculates gradient at anticipated future position.", results: "Matched adaptive optimizers when properly tuned.", strengths: "Often better generalization than Adam, simple, well-understood.", limitations: "Requires careful hyperparameter tuning, learning rate scheduling.", whoShouldRead: ["researcher", "interview prep"], tldr: "Don't underestimate SGD + momentum. With proper tuning, it competes with anything.", keyInsight: "Nesterov evaluates gradient at θ + μv (lookahead) instead of θ — reduces oscillation.", link: "http://proceedings.mlr.press/v28/sutskever13.html" }),
      makePaper({ id: "opt3", title: "AdamW: Decoupled Weight Decay Regularization", year: 2019, difficulty: "Intermediate", authors: "Loshchilov, Hutter", institution: "University of Freiburg", citations: 15000, section: "improvement", whyItMatters: "Fixed how Adam handles weight decay. Now standard for training LLMs.", problem: "L2 regularization in Adam is weakened by adaptive rates. How to properly combine them?", coreContribution: "Separate weight decay from gradient update so decay works uniformly.", methodIntuition: "Two forces: learning (gradients) and shrinkage (decay). Adam couples them; AdamW keeps them independent.", results: "Improved generalization for BERT, GPT, and most transformers.", strengths: "Correct weight decay, better generalization, simple fix.", limitations: "Still requires weight decay coefficient tuning.", whoShouldRead: ["researcher", "interview prep"], tldr: "Adam + properly decoupled weight decay. A subtle fix behind most modern LLM training.", keyInsight: "Apply decay directly (θ = θ - λθ) instead of adding to gradient. Uniform decay regardless of gradient magnitude.", link: "https://arxiv.org/abs/1711.05101" }),
      makePaper({ id: "opt4", title: "Learning Rate Schedules: Cosine Annealing and Warm-up", year: 2017, difficulty: "Beginner", authors: "Loshchilov, Hutter / Goyal et al.", institution: "Various", citations: 10000, section: "improvement", whyItMatters: "Learning rate scheduling is often more important than optimizer choice.", problem: "Fixed learning rates are suboptimal. How should learning rate change during training?", coreContribution: "Cosine annealing smoothly decreases LR; warm-up prevents instability at start of training.", methodIntuition: "Start slow (warm-up: let the model find the right direction), then decrease smoothly (cosine: take finer steps as you approach the minimum).", results: "Standard in all modern training pipelines. Warm-up is essential for large batch training.", strengths: "Simple to implement, consistently improves results, no extra hyperparameters.", limitations: "Cosine schedule assumes you know total training steps; warm-up length is a hyperparameter.", whoShouldRead: ["beginner", "interview prep"], tldr: "Warm up, then cosine decay. Simple scheduling that consistently improves training.", keyInsight: "Warm-up prevents early training instability with large learning rates — the initial random weights need gentle gradients to find a reasonable direction first.", link: "https://arxiv.org/abs/1608.03983" }),
      makePaper({ id: "opt5", title: "Gradient Clipping for Stable Training", year: 2013, difficulty: "Beginner", authors: "Pascanu, Mikolov, Bengio", institution: "Université de Montréal", citations: 8000, section: "foundation", whyItMatters: "Gradient clipping prevents exploding gradients — essential for RNN and transformer training.", problem: "Gradients can explode during backpropagation, causing NaN losses and training failure.", coreContribution: "Clip gradient norm to a threshold: if ||g|| > threshold, scale g down to threshold.", methodIntuition: "Put a speed limit on gradient updates. The direction stays the same but magnitude is capped.", results: "Enabled stable training of RNNs and transformers. Now standard in all LLM training.", strengths: "Simple, effective, minimal overhead, preserves gradient direction.", limitations: "Threshold is a hyperparameter; aggressive clipping can slow convergence.", whoShouldRead: ["beginner", "interview prep"], tldr: "Cap gradient magnitude, keep direction. Prevents exploding gradients. Essential for stable training.", keyInsight: "Clipping the global norm (not per-parameter) preserves the relative gradient scales between parameters — this is critical for maintaining learning dynamics.", link: "https://arxiv.org/abs/1211.5063" }),
      makePaper({ id: "opt6", title: "LAMB: Large Batch Training of BERT", year: 2020, difficulty: "Advanced", authors: "You, Li, Reddi et al.", institution: "Google / UC Berkeley", citations: 3500, section: "modern", whyItMatters: "Enabled BERT training with batch sizes up to 65K — reducing training from 3 days to 76 minutes.", problem: "Large batches degrade model quality due to optimization challenges. How to scale batch size to extremes?", coreContribution: "Layer-wise adaptive learning rates that normalize gradient updates by both first and second moments AND by layer-specific parameter norms.", methodIntuition: "Each layer gets its own 'speed limit' based on how big its weights are — large layers take proportionally larger steps.", results: "Trained BERT in 76 minutes on TPU pods vs 3 days originally. Linear scaling efficiency.", strengths: "Enables massive parallelism, scales nearly linearly, maintains quality.", limitations: "Requires very large compute infrastructure; not beneficial for small-batch training.", whoShouldRead: ["researcher"], tldr: "Layer-wise adaptive rates enable massive batch training. BERT in 76 minutes instead of 3 days.", keyInsight: "The trust ratio ||θ||/||update|| normalizes updates per-layer — layers with large weights aren't destabilized by proportionally small gradients.", link: "https://arxiv.org/abs/1904.00962" }),
      makePaper({ id: "opt7", title: "Lookahead Optimizer: K Steps Forward, 1 Step Back", year: 2019, difficulty: "Intermediate", authors: "Zhang, Lucas, Hinton, Ba", institution: "University of Toronto", citations: 2500, section: "modern", whyItMatters: "A meta-optimizer that wraps around any base optimizer to reduce variance and improve stability.", problem: "Fast optimizers can oscillate or explore unstable regions. How to get exploration without instability?", coreContribution: "Maintain slow weights; let fast optimizer explore for k steps, then interpolate back toward slow weights.", methodIntuition: "Send a scout ahead for k steps, then move the team partway toward the scout's position. If the scout went wrong, the team barely moves.", results: "Improved convergence stability across various tasks with minimal overhead.", strengths: "Wraps any optimizer, reduces variance, almost no hyperparameters.", limitations: "Marginal gains on well-tuned baselines; adds slight memory overhead.", whoShouldRead: ["researcher"], tldr: "Let optimizer explore, then pull back toward a stable trajectory. Reduces variance, improves stability.", keyInsight: "The slow weights act as a Polyak averaging variant — they provide a stable anchor that prevents the fast optimizer's exploration from going too far astray.", link: "https://arxiv.org/abs/1907.08610" }),
      makePaper({ id: "opt8", title: "Sharpness-Aware Minimization (SAM)", year: 2021, difficulty: "Advanced", authors: "Foret, Kleiner, Mobahi, Neyshabur", institution: "Google Research", citations: 4000, section: "modern", whyItMatters: "SAM finds flatter minima that generalize better — connecting optimization geometry to generalization.", problem: "Standard optimizers find sharp minima that don't generalize well. Can we explicitly seek flat minima?", coreContribution: "Minimize the worst-case loss in a neighborhood of the current weights: min_w max_{||ε||≤ρ} L(w+ε).", methodIntuition: "Don't just find a low valley — find a WIDE valley. A marble at the bottom of a wide valley stays put when shaken (generalizes). In a narrow valley it rolls out (overfits).", results: "Consistent 0.5-2% accuracy improvement across benchmarks. SOTA on CIFAR and ImageNet with standard architectures.", strengths: "Simple implementation, consistent gains, strong generalization theory.", limitations: "2x compute per step (two forward-backward passes); ρ requires tuning.", whoShouldRead: ["researcher", "interview prep"], tldr: "Seek flat minima by minimizing worst-case loss in a neighborhood. Better generalization, simple idea.", keyInsight: "The perturbation direction is approximated by the gradient itself: ε ≈ ρ·∇L/||∇L|| — this makes SAM a 'gradient of the gradient' method.", link: "https://arxiv.org/abs/2010.01412" }),
    ],
  },
  {
    keywords: ["regression", "linear regression", "logistic regression", "prediction", "ridge", "lasso"],
    papers: [
      makePaper({ id: "reg1", title: "Linear Regression and the Method of Least Squares", year: 2001, difficulty: "Beginner", authors: "Hastie, Tibshirani, Friedman", institution: "Stanford", citations: 65000, section: "foundation", whyItMatters: "The foundation of all supervised learning — understanding it deeply prepares you for everything.", problem: "How to find the best linear relationship and prevent overfitting?", coreContribution: "Unified treatment of regression, Ridge, and Lasso regularization with bias-variance tradeoff.", methodIntuition: "Draw the best line through points. Add penalty for steep slopes to prefer simpler models.", results: "Established theoretical foundation for all ML practitioners.", strengths: "Interpretable, fast, closed-form solution, strong theoretical basis.", limitations: "Assumes linearity, sensitive to outliers (use robust regression).", whoShouldRead: ["beginner", "interview prep"], tldr: "Fit a line, minimize squared errors, add penalty for complexity. Every ML career starts here.", keyInsight: "Ridge shrinks all coefficients toward zero; Lasso can shrink to exactly zero (feature selection). Huge practical difference.", link: "https://hastie.su.domains/ElemStatLearn/" }),
      makePaper({ id: "reg2", title: "Logistic Regression: Theory and Practice", year: 1958, difficulty: "Beginner", authors: "Cox, Walker", institution: "Various", citations: 40000, section: "foundation", whyItMatters: "Bridges regression and classification. Each neuron is a tiny logistic regression unit.", problem: "How to model probability of class membership from linear outputs?", coreContribution: "Sigmoid transforms linear outputs to probabilities. Maximum likelihood for training.", methodIntuition: "Instead of a line, draw a smooth S-curve. Below 0.5 → class 0, above → class 1.", results: "Standard baseline in industry. Used daily in medicine, finance, A/B testing.", strengths: "Interpretable coefficients, fast, probabilistic outputs, well-calibrated.", limitations: "Linear decision boundary, can't capture complex patterns without feature engineering.", whoShouldRead: ["beginner", "interview prep"], tldr: "Apply sigmoid to linear model, get probabilities. First classifier to learn, last to stop using.", keyInsight: "log(p/(1-p)) = wx + b — each unit increase in x multiplies odds by e^w. Directly interpretable.", link: "https://www.jstor.org/stable/2346" }),
      makePaper({ id: "reg3", title: "Elastic Net: Combining Ridge and Lasso", year: 2005, difficulty: "Intermediate", authors: "Zou, Hastie", institution: "Stanford", citations: 20000, section: "improvement", whyItMatters: "Best of both: feature selection (Lasso) + grouped selection (Ridge) for correlated features.", problem: "Lasso picks one from correlated features, ignoring rest. Ridge can't select. Can we get both?", coreContribution: "Convex combination of L1+L2 penalties for grouped feature selection.", methodIntuition: "Lasso picks one star per friend group. Ridge keeps all but weakens. Elastic Net picks groups together.", results: "Default regularization in sklearn for high-dimensional data.", strengths: "Handles correlations, performs selection, group stability.", limitations: "Two hyperparameters to tune (α and λ).", whoShouldRead: ["researcher", "interview prep"], tldr: "L1 + L2 = feature selection + group selection. Go-to for correlated features.", keyInsight: "Mixing parameter α between 0.1-0.9 captures benefits of both.", link: "https://academic.oup.com/jrsssb/article/67/2/301/7109482" }),
      makePaper({ id: "reg4", title: "Polynomial Regression and the Bias-Variance Tradeoff", year: 1998, difficulty: "Beginner", authors: "Geman, Bienenstock, Doursat", institution: "Brown University", citations: 8000, section: "foundation", whyItMatters: "The bias-variance tradeoff is the most fundamental concept in ML — polynomial regression makes it tangible.", problem: "Why do complex models overfit and simple ones underfit? What's the optimal complexity?", coreContribution: "Decomposing prediction error into bias² + variance + noise, showing the fundamental tradeoff.", methodIntuition: "Fitting a straight line to a curve = too simple (high bias). Fitting a degree-100 polynomial = too complex (high variance). The sweet spot is in between.", results: "Provided the conceptual framework used to understand every ML algorithm's behavior.", strengths: "Fundamental theoretical insight, applies to all ML methods.", limitations: "The decomposition is for squared loss only; real tradeoffs are more nuanced.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "Simple models underfit (bias), complex ones overfit (variance). Find the sweet spot.", keyInsight: "Error = Bias² + Variance + Noise. You can decrease one but the other increases. The noise term is irreducible — no model can beat it.", link: "https://doi.org/10.1162/neco.1992.4.1.1" }),
      makePaper({ id: "reg5", title: "Bayesian Linear Regression", year: 2006, difficulty: "Intermediate", authors: "Bishop", institution: "Microsoft Research Cambridge", citations: 15000, section: "improvement", whyItMatters: "Bayesian regression provides uncertainty estimates — crucial for safety-critical applications.", problem: "Standard regression gives point predictions. How confident should we be? When is the model uncertain?", coreContribution: "Place priors on weights, compute posterior distributions, get prediction intervals for free.", methodIntuition: "Instead of finding THE best line, find a distribution over all plausible lines. Where they agree = confident. Where they spread = uncertain.", results: "Standard in scientific applications where uncertainty quantification is critical.", strengths: "Natural uncertainty quantification, built-in regularization via priors, no overfitting.", limitations: "Computationally expensive for large models; choice of prior can be subjective.", whoShouldRead: ["researcher", "interview prep"], tldr: "Distributions over predictions instead of point estimates. Know when your model is uncertain.", keyInsight: "The posterior is proportional to likelihood × prior — data updates your beliefs. With enough data, the posterior converges regardless of prior.", link: "https://www.microsoft.com/en-us/research/publication/pattern-recognition-and-machine-learning/" }),
      makePaper({ id: "reg6", title: "Quantile Regression for Robust Prediction", year: 1978, difficulty: "Intermediate", authors: "Koenker, Bassett", institution: "University of Illinois", citations: 15000, section: "improvement", whyItMatters: "Estimates conditional quantiles instead of means — robust to outliers and gives prediction intervals.", problem: "Mean regression is dominated by outliers. How to predict medians or arbitrary quantiles?", coreContribution: "Minimize asymmetric loss that weights positive and negative errors differently per quantile.", methodIntuition: "Instead of 'what's the average outcome?', ask 'what's the 10th percentile? 90th percentile?' This gives you the full range of likely outcomes.", results: "Standard in economics, finance, and weather forecasting where tail behavior matters.", strengths: "Robust to outliers, provides prediction intervals, no distributional assumptions.", limitations: "Quantile crossing can occur; less efficient than OLS when normality holds.", whoShouldRead: ["researcher"], tldr: "Predict quantiles instead of means. Robust, informative, handles asymmetric outcomes.", keyInsight: "The check loss ρ_τ(u) = u(τ - I(u<0)) elegantly weights over/under-predictions differently — τ=0.5 gives the median, which is robust to outliers.", link: "https://doi.org/10.2307/1913643" }),
      makePaper({ id: "reg7", title: "Gradient Boosted Regression Trees (GBRT)", year: 2001, difficulty: "Intermediate", authors: "Friedman", institution: "Stanford", citations: 25000, section: "improvement", whyItMatters: "GBRT is the most powerful general-purpose regression method for tabular data.", problem: "Linear models are too simple for complex relationships. How to build flexible non-linear regression?", coreContribution: "Sequentially fit trees to residuals, building an additive model that captures complex non-linear patterns.", methodIntuition: "First tree captures the main trend. Second tree fixes the biggest remaining errors. Third tree fixes what's still wrong. Each tree polishes what the last one missed.", results: "Dominates tabular ML benchmarks. Backbone of XGBoost, LightGBM, CatBoost.", strengths: "Handles non-linearity, feature interactions, missing data, mixed types.", limitations: "Sequential training (can't parallelize trees), requires regularization to avoid overfitting.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "Fit trees to residuals, stack them up. The most powerful tabular regression method.", keyInsight: "The learning rate (shrinkage) is critical: smaller steps = more trees needed but better generalization. It's the ensemble equivalent of weight decay.", link: "https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full" }),
      makePaper({ id: "reg8", title: "Neural Network Regression and Universal Function Approximation", year: 2017, difficulty: "Advanced", authors: "Goodfellow, Bengio, Courville", institution: "Various", citations: 45000, section: "modern", whyItMatters: "Understanding when neural networks outperform classical regression methods and why.", problem: "When should you use a neural network for regression instead of classical methods?", coreContribution: "Framework for understanding when deep models provide genuine benefits over shallow methods for regression.", methodIntuition: "Classical regression is like fitting shapes with a ruler and compass. Neural regression is like sculpting clay — more flexible but needs more data and skill.", results: "Clear guidelines: NNs excel when data is abundant, features have spatial/temporal structure, and relationships are highly non-linear.", strengths: "Captures arbitrarily complex patterns, end-to-end feature learning, scales with data.", limitations: "Requires large data, harder to interpret, more expensive to train than classical methods.", whoShouldRead: ["researcher", "interview prep"], tldr: "Neural nets for regression: maximum flexibility, maximum data hunger. Use when classical methods aren't enough.", keyInsight: "The advantage of deep regression over kernel methods isn't approximation power (both are universal) — it's that deep models learn hierarchical features, composing simple functions into complex ones.", link: "https://www.deeplearningbook.org/" }),
    ],
  },
  {
    keywords: ["nlp", "natural language", "language model", "text", "word embedding", "word2vec", "embedding"],
    papers: [
      makePaper({ id: "nlp1", title: "Word2Vec: Efficient Estimation of Word Representations", year: 2013, difficulty: "Beginner", authors: "Mikolov, Chen, Corrado, Dean", institution: "Google", citations: 40000, section: "foundation", whyItMatters: "'King - man + woman = queen' showed words could encode meaning as vectors.", problem: "One-hot encoding treats all words as equally different. How to capture semantic similarity?", coreContribution: "Skip-gram and CBOW models learning embeddings from context prediction.", methodIntuition: "You know a word by the company it keeps. Similar contexts → similar vectors.", results: "Captured analogies, synonymy, geographic relationships. Transformed NLP input representations.", strengths: "Simple, fast, captures semantic relationships, widely applicable.", limitations: "One vector per word (no polysemy), no subword information, static embeddings.", whoShouldRead: ["beginner", "interview prep"], tldr: "Learn word vectors from context. Similar meanings = similar vectors.", keyInsight: "king - man + woman ≈ queen because gender direction is consistent in embedding space.", link: "https://arxiv.org/abs/1301.3781" }),
      makePaper({ id: "nlp2", title: "GloVe: Global Vectors for Word Representation", year: 2014, difficulty: "Intermediate", authors: "Pennington, Socher, Manning", institution: "Stanford NLP Group", citations: 30000, section: "foundation", whyItMatters: "Combined count-based and prediction-based approaches for richer embeddings.", problem: "Word2Vec uses only local context. Can we leverage global co-occurrence statistics?", coreContribution: "Weighted least-squares on global co-occurrence matrix.", methodIntuition: "Count how often every pair appears together corpus-wide. Find vectors whose dot products match.", results: "Outperformed Word2Vec on analogy tasks. Competitive downstream.", strengths: "Captures global statistics, efficient training, interpretable objective.", limitations: "Static embeddings, large co-occurrence matrix, no subword info.", whoShouldRead: ["researcher"], tldr: "Global co-occurrence + vector learning = rich word embeddings.", keyInsight: "Ratios of co-occurrence probabilities are more informative than raw counts.", link: "https://nlp.stanford.edu/pubs/glove.pdf" }),
      makePaper({ id: "nlp3", title: "Seq2Seq: Sequence to Sequence Learning", year: 2014, difficulty: "Intermediate", authors: "Sutskever, Vinyals, Le", institution: "Google", citations: 20000, section: "foundation", whyItMatters: "Encoder-decoder architecture made neural translation practical.", problem: "How to map variable-length input to variable-length output?", coreContribution: "Encoder LSTM compresses input into vector, decoder LSTM unpacks into output.", methodIntuition: "One network reads and compresses meaning. A second network unpacks word by word.", results: "Near SOTA on English-French translation. Inspired chatbots, summarization, code gen.", strengths: "End-to-end training, handles variable lengths, general framework.", limitations: "Fixed bottleneck vector, struggles with long sequences (solved by attention).", whoShouldRead: ["beginner", "researcher"], tldr: "Encode→vector→decode. Simple architecture for sequence mapping.", keyInsight: "Reversing input sequence creates short-term dependencies that LSTMs handle much better.", link: "https://arxiv.org/abs/1409.3215" }),
      makePaper({ id: "nlp4", title: "Attention Mechanism for Neural Machine Translation", year: 2015, difficulty: "Intermediate", authors: "Bahdanau, Cho, Bengio", institution: "Université de Montréal", citations: 35000, section: "improvement", whyItMatters: "Attention solved the fixed bottleneck of Seq2Seq and later became the core of Transformers.", problem: "The fixed-size context vector can't capture long sequences. How to attend to relevant parts?", coreContribution: "Learned attention weights that let the decoder focus on different input positions at each step.", methodIntuition: "Instead of compressing the whole book into one summary, let the reader flip back to relevant pages at each step.", results: "Significant improvement on long sentences. Enabled attention-based architectures.", strengths: "Soft alignment, interpretable attention maps, handles long sequences.", limitations: "O(n×m) computation for source×target; attention can be noisy.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "Let the decoder look back at the input. Attention solves the bottleneck.", keyInsight: "Attention weights are a soft alignment — they show which source words are relevant for each target word, providing interpretability for free.", link: "https://arxiv.org/abs/1409.0473" }),
      makePaper({ id: "nlp5", title: "Byte Pair Encoding (BPE) for Subword Tokenization", year: 2016, difficulty: "Beginner", authors: "Sennrich, Haddow, Birch", institution: "University of Edinburgh", citations: 10000, section: "improvement", whyItMatters: "BPE solved the open-vocabulary problem — handle any word, even unseen ones, by breaking into subwords.", problem: "Word-level models can't handle unseen words. Character-level models are too slow. How to balance?", coreContribution: "Iteratively merge the most frequent character pairs to build a subword vocabulary.", methodIntuition: "Start with individual characters. Merge the most common pairs repeatedly: 't'+'h'→'th', 'th'+'e'→'the'. Common words stay whole, rare words decompose into known pieces.", results: "Became the standard tokenization for all modern LLMs (GPT, BERT, etc.).", strengths: "Handles any word, compact vocabulary, balances coverage and granularity.", limitations: "Tokenization is language-specific; optimal vocab size is a hyperparameter.", whoShouldRead: ["beginner", "interview prep"], tldr: "Merge frequent character pairs into subwords. Handle any word. The tokenizer behind all LLMs.", keyInsight: "BPE naturally creates morphologically meaningful units — 'un', 'break', 'able' often emerge as separate tokens, capturing linguistic structure.", link: "https://arxiv.org/abs/1508.07909" }),
      makePaper({ id: "nlp6", title: "Sentence-BERT: Sentence Embeddings via Siamese Networks", year: 2019, difficulty: "Intermediate", authors: "Reimers, Gurevych", institution: "TU Darmstadt", citations: 8000, section: "modern", whyItMatters: "Made semantic search and sentence comparison practical — BERT alone was too slow for pairwise comparison.", problem: "BERT requires feeding both sentences together — O(n²) for comparing n sentences. How to get fast embeddings?", coreContribution: "Fine-tune BERT with siamese/triplet networks to produce fixed-size sentence embeddings.", methodIntuition: "Train BERT to compress each sentence into a single vector such that similar sentences have nearby vectors. Then compare millions of sentences by simple vector distance.", results: "Enabled semantic search, clustering, and deduplication at scale. Foundation for RAG systems.", strengths: "Fast inference (one pass per sentence), composable, semantic similarity.", limitations: "Fixed-size embedding loses some nuance; domain adaptation may be needed.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "Turn sentences into vectors. Compare millions by simple distance. Foundation for semantic search.", keyInsight: "The key is contrastive training — push similar sentences together and dissimilar ones apart in embedding space.", link: "https://arxiv.org/abs/1908.10084" }),
      makePaper({ id: "nlp7", title: "Chain-of-Thought Prompting for LLM Reasoning", year: 2022, difficulty: "Beginner", authors: "Wei, Wang, Schuurmans et al.", institution: "Google Brain", citations: 6000, section: "modern", whyItMatters: "Showed that asking LLMs to 'think step by step' dramatically improves reasoning accuracy.", problem: "LLMs fail at multi-step reasoning when asked directly. How to unlock reasoning without fine-tuning?", coreContribution: "Include step-by-step reasoning examples in prompts. The model follows the reasoning pattern.", methodIntuition: "Instead of asking a student for the final answer, ask them to show their work. The act of writing out steps helps them reason correctly.", results: "18% improvement on math problems, 40%+ on some reasoning benchmarks. Works with zero-shot 'Let's think step by step.'", strengths: "No training needed, works on any LLM, dramatically improves reasoning.", limitations: "Increases token usage, doesn't guarantee correct reasoning, can produce plausible but wrong chains.", whoShouldRead: ["beginner", "interview prep"], tldr: "'Let's think step by step' — four words that make LLMs dramatically smarter at reasoning.", keyInsight: "The reasoning chain isn't just output formatting — the model actually uses intermediate steps as 'working memory' to solve harder problems.", link: "https://arxiv.org/abs/2201.11903" }),
      makePaper({ id: "nlp8", title: "Retrieval-Augmented Generation (RAG)", year: 2020, difficulty: "Intermediate", authors: "Lewis, Perez, Piktus et al.", institution: "Facebook AI / UCL", citations: 5000, section: "modern", whyItMatters: "RAG grounds LLM responses in retrieved documents, reducing hallucination and enabling knowledge updates without retraining.", problem: "LLMs hallucinate and their knowledge is frozen at training time. How to give them access to current, verified information?", coreContribution: "Retrieve relevant documents at inference time and condition the generation on retrieved passages.", methodIntuition: "Instead of answering from memory (which might be wrong), look up the answer in a reference book first, then compose your response. Open-book exam vs closed-book.", results: "Reduced hallucination, enabled domain-specific QA, became standard for enterprise AI.", strengths: "Grounded in sources, updatable knowledge, attributable answers.", limitations: "Quality depends on retrieval; adds latency; chunk boundaries can break context.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "Retrieve relevant docs, feed to LLM, ground the response. The architecture behind enterprise AI.", keyInsight: "The retriever and generator can be trained jointly — the retriever learns to find passages that help the generator produce correct answers.", link: "https://arxiv.org/abs/2005.11401" }),
    ],
  },
  {
    keywords: ["diffusion", "stable diffusion", "image synthesis", "denoising", "dall-e", "text-to-image"],
    papers: [
      makePaper({ id: "diff1", title: "Denoising Diffusion Probabilistic Models (DDPM)", year: 2020, difficulty: "Advanced", authors: "Ho, Jain, Abbeel", institution: "UC Berkeley", citations: 15000, section: "foundation", whyItMatters: "Foundation of Stable Diffusion, DALL-E, Midjourney. The AI art revolution.", problem: "GANs are unstable and collapse. Is there a stable, high-quality generative approach?", coreContribution: "Add noise gradually, learn to reverse it step by step. Stable training with simple MSE loss.", methodIntuition: "Dissolve a photo into static over 1000 steps. Train to reverse each tiny step. Run backwards from noise → image.", results: "Matched/exceeded GANs with no mode collapse, no instability.", strengths: "Stable training, diverse outputs, high quality, simple loss.", limitations: "Slow sampling (many steps), high compute for training.", whoShouldRead: ["researcher", "interview prep"], tldr: "Add noise, learn to reverse it. Stable, diverse, stunning quality.", keyInsight: "Each step only removes a tiny amount of noise — 1000 easy steps compose into one hard generation.", link: "https://arxiv.org/abs/2006.11239" }),
      makePaper({ id: "diff2", title: "Latent Diffusion Models (Stable Diffusion)", year: 2022, difficulty: "Advanced", authors: "Rombach, Blattmann, Lorenz et al.", institution: "LMU Munich / Runway", citations: 12000, section: "improvement", whyItMatters: "Made high-res generation practical on consumer GPUs.", problem: "Pixel-space diffusion at 512×512 is too expensive. How to maintain quality cheaply?", coreContribution: "Diffuse in autoencoder's latent space (64×64 instead of 512×512) with cross-attention for text.", methodIntuition: "Edit a compressed JPEG instead of a raw file — much faster, same visual quality.", results: "First open-source text-to-image model on consumer GPUs. Massive creative community.", strengths: "Fast, high quality, text-conditioned, open source.", limitations: "Autoencoder bottleneck, potential artifacts at compression boundary.", whoShouldRead: ["researcher"], tldr: "Diffusion in latent space = fast + high quality. Powers Stable Diffusion.", keyInsight: "The autoencoder's latent space is trained to preserve perceptually meaningful changes.", link: "https://arxiv.org/abs/2112.10752" }),
      makePaper({ id: "diff3", title: "Classifier-Free Diffusion Guidance", year: 2022, difficulty: "Advanced", authors: "Ho, Salimans", institution: "Google Brain", citations: 6000, section: "improvement", whyItMatters: "Why text-to-image follows your prompts well. The quality knob for AI image generation.", problem: "How to make diffusion models follow text more faithfully without a separate classifier?", coreContribution: "Train one model conditionally and unconditionally, amplify the difference.", methodIntuition: "'What comes from noise?' vs 'What matching this text comes from noise?' Amplify the difference.", results: "Dramatically improved text-image alignment in DALL-E 2, Stable Diffusion, Imagen.", strengths: "Single model, controllable quality knob, no extra classifier.", limitations: "High guidance = oversaturation; requires training with dropout of conditioning.", whoShouldRead: ["researcher"], tldr: "Subtract unconditional from conditional, amplify. Makes AI images match text.", keyInsight: "ε_guided = ε_uncond + s·(ε_cond - ε_uncond). s=7-15 is the sweet spot.", link: "https://arxiv.org/abs/2207.12598" }),
      makePaper({ id: "diff4", title: "DDIM: Denoising Diffusion Implicit Models", year: 2021, difficulty: "Advanced", authors: "Song, Meng, Ermon", institution: "Stanford", citations: 7000, section: "improvement", whyItMatters: "Made diffusion 10-50x faster by enabling deterministic sampling with fewer steps.", problem: "DDPM needs 1000 steps to generate an image. Can we reduce this dramatically?", coreContribution: "Non-Markovian process that allows deterministic sampling and skipping steps.", methodIntuition: "Instead of taking 1000 tiny steps, take 50 bigger steps along the same path. Same destination, 20x faster.", results: "10-50 step generation with minimal quality loss. Enabled practical diffusion applications.", strengths: "Much faster, deterministic (same noise → same image), interpolatable.", limitations: "Slight quality loss vs full DDPM; still iterative.", whoShouldRead: ["researcher"], tldr: "Skip diffusion steps deterministically. 20x faster, nearly same quality.", keyInsight: "The key insight is that the diffusion process doesn't need to be Markovian — the non-Markovian formulation allows arbitrary step skipping.", link: "https://arxiv.org/abs/2010.02502" }),
      makePaper({ id: "diff5", title: "ControlNet: Adding Spatial Control to Diffusion Models", year: 2023, difficulty: "Advanced", authors: "Zhang, Rao, Agrawala", institution: "Stanford", citations: 5000, section: "modern", whyItMatters: "Made diffusion models controllable — generate images following edges, poses, depth maps.", problem: "Text prompts give high-level control but no spatial precision. How to control exact layout?", coreContribution: "Clone and fine-tune a locked copy of the diffusion model that accepts spatial conditions (edges, pose, depth).", methodIntuition: "The original model knows how to paint. ControlNet adds a guide rail — 'paint WITHIN these lines.'", results: "Precise control over generated images using edge maps, segmentation, pose skeletons, depth.", strengths: "Precise spatial control, preserves model quality, modular (stackable).", limitations: "Requires training per condition type; adds inference cost.", whoShouldRead: ["researcher"], tldr: "Guide diffusion with spatial conditions — edges, poses, depth. Precision + creativity.", keyInsight: "Locking the original weights prevents catastrophic forgetting — the cloned 'control' branch learns spatial conditioning without degrading generation quality.", link: "https://arxiv.org/abs/2302.05543" }),
      makePaper({ id: "diff6", title: "Flow Matching and Rectified Flows", year: 2023, difficulty: "Advanced", authors: "Lipman, Chen, Ben-Hamu et al.", institution: "Meta AI / Various", citations: 3000, section: "modern", whyItMatters: "Simplified diffusion theory — learn straight paths from noise to data instead of curved diffusion processes.", problem: "Diffusion models use complex noise schedules. Is there a simpler generative framework?", coreContribution: "Learn a velocity field that transports noise to data along straight paths — simpler theory, fewer steps.", methodIntuition: "Instead of a complex winding path from noise to image, draw a straight line and learn to follow it.", results: "Competitive quality with fewer sampling steps. Powers Stable Diffusion 3.", strengths: "Simpler theory, straighter paths = fewer steps, flexible framework.", limitations: "Still requires many function evaluations; theory is newer and less established.", whoShouldRead: ["researcher"], tldr: "Straight paths from noise to data. Simpler than diffusion, fewer steps. The next generation.", keyInsight: "Optimal transport gives the straightest paths — when paths don't cross, each step makes maximum progress toward the target.", link: "https://arxiv.org/abs/2210.02747" }),
      makePaper({ id: "diff7", title: "DALL-E and Text-Conditioned Image Generation", year: 2021, difficulty: "Intermediate", authors: "Ramesh, Pavlov, Goh et al.", institution: "OpenAI", citations: 8000, section: "foundation", whyItMatters: "Demonstrated that AI can create images from arbitrary text descriptions — 'an armchair shaped like an avocado.'", problem: "Can a model generate coherent, creative images from arbitrary natural language descriptions?", coreContribution: "Autoregressive model that generates image tokens conditioned on text tokens via a transformer.", methodIntuition: "Just as GPT generates text word by word, DALL-E generates image patches token by token, guided by the text description.", results: "Generated creative, coherent images from arbitrary text. Launched the text-to-image revolution.", strengths: "Arbitrary text conditioning, creative composition, zero-shot generation.", limitations: "Autoregressive is slow; diffusion-based DALL-E 2/3 later proved superior.", whoShouldRead: ["beginner", "researcher"], tldr: "Text → image via autoregressive generation. 'An armchair shaped like an avocado' actually works.", keyInsight: "CLIP re-ranking is crucial — generate many candidates, then use CLIP to select the one that best matches the text.", link: "https://arxiv.org/abs/2102.12092" }),
      makePaper({ id: "diff8", title: "Consistency Models: Fast Single-Step Generation", year: 2023, difficulty: "Advanced", authors: "Song, Dhariwal, Chen, Sutskever", institution: "OpenAI", citations: 2000, section: "modern", whyItMatters: "Enable single-step generation from diffusion models — turning iterative into instant.", problem: "Even fast samplers need 10+ steps. Can we generate in a single step?", coreContribution: "Train a model to map any point on the diffusion trajectory directly to the clean image.", methodIntuition: "Instead of walking the full path from noise to image, learn to teleport directly to the destination from any starting point.", results: "Competitive quality in 1-2 steps. Enables real-time generation applications.", strengths: "Single-step generation, real-time capable, distills full model.", limitations: "Slight quality loss vs multi-step; training requires careful consistency enforcement.", whoShouldRead: ["researcher"], tldr: "One step from noise to image. Diffusion quality at real-time speed.", keyInsight: "The self-consistency property: any two points on the same trajectory should map to the same output. This constraint is sufficient to learn generation.", link: "https://arxiv.org/abs/2303.01469" }),
    ],
  },
  {
    keywords: ["recurrent", "rnn", "lstm", "gru", "sequence", "time series"],
    papers: [
      makePaper({ id: "rnn1", title: "Long Short-Term Memory (LSTM)", year: 1997, difficulty: "Intermediate", authors: "Hochreiter, Schmidhuber", institution: "TU Munich", citations: 75000, section: "foundation", whyItMatters: "Solved the vanishing gradient problem for RNNs. Dominated sequence modeling for 20 years.", problem: "RNNs forget over long sequences due to vanishing gradients. How to create persistent memory?", coreContribution: "Gated cell with input, forget, and output gates controlling information flow.", methodIntuition: "A conveyor belt with workers who decide what to add, remove, and report. The belt carries long-term memory.", results: "SOTA on speech, language, handwriting. Dominant until transformers.", strengths: "Long-range dependencies, gradient-stable, versatile.", limitations: "Sequential computation, slower than transformers, complex architecture.", whoShouldRead: ["beginner", "researcher", "interview prep"], tldr: "Gates control memory: remember, forget, output. Foundation of sequence modeling.", keyInsight: "Cell state flows with only linear interactions — constant error carousel prevents gradient vanishing over 1000+ steps.", link: "https://www.bioinf.jku.at/publications/older/2604.pdf" }),
      makePaper({ id: "rnn2", title: "GRU: Gated Recurrent Unit", year: 2014, difficulty: "Intermediate", authors: "Cho, Van Merriënboer, Gulcehre et al.", institution: "Université de Montréal", citations: 20000, section: "improvement", whyItMatters: "Simplified LSTM with two gates instead of three. Comparable performance, fewer parameters.", problem: "LSTMs have three gates and separate cell state. Is all this complexity necessary?", coreContribution: "Two gates (reset, update) merging cell and hidden state.", methodIntuition: "One dial between 'remember everything' and 'start fresh.' Simpler than LSTM's three-gate system.", results: "Matched LSTM on most benchmarks while being faster to train.", strengths: "Fewer parameters, faster, comparable performance.", limitations: "Slightly less flexible than LSTM on some tasks; still sequential.", whoShouldRead: ["beginner", "interview prep"], tldr: "Two gates instead of three. Simpler, faster, nearly as powerful.", keyInsight: "z·old + (1-z)·new: one gate controls both forgetting and input.", link: "https://arxiv.org/abs/1406.1078" }),
      makePaper({ id: "rnn3", title: "Bidirectional RNNs for Sequence Labeling", year: 1997, difficulty: "Beginner", authors: "Schuster, Paliwal", institution: "Various", citations: 10000, section: "foundation", whyItMatters: "Using both past and future context. The idea persists in BERT.", problem: "Standard RNNs only see past context. How to use future information too?", coreContribution: "Forward + backward RNNs concatenated at each position.", methodIntuition: "Read left-to-right AND right-to-left. Combine both views for complete context.", results: "Improved POS tagging, NER, phoneme classification.", strengths: "Complete context, simple architecture, composable.", limitations: "Can't be used for autoregressive generation; doubles parameters.", whoShouldRead: ["beginner", "interview prep"], tldr: "Process forward and backward, combine. Double the context.", keyInsight: "Same insight as BERT's masked LM — bidirectional context gives richer representations.", link: "https://ieeexplore.ieee.org/document/650093" }),
      makePaper({ id: "rnn4", title: "Attention-based RNNs and Temporal Attention", year: 2016, difficulty: "Intermediate", authors: "Various", institution: "Various", citations: 12000, section: "improvement", whyItMatters: "Applied attention to time series, enabling RNNs to focus on relevant historical time steps.", problem: "Fixed-length RNN summaries can't capture which historical moments matter most for current predictions.", coreContribution: "Temporal attention mechanism that weights historical hidden states by relevance to current prediction.", methodIntuition: "When predicting tomorrow's stock price, not all past days matter equally. Attention lets the model focus on the most relevant historical patterns.", results: "Significant improvements in time series forecasting, clinical event prediction, and speech recognition.", strengths: "Interpretable (which time steps matter), handles long sequences, selective memory.", limitations: "Additional computation per step; attention patterns can be noisy.", whoShouldRead: ["researcher"], tldr: "Let RNNs attend to the most relevant past moments. Selective memory for time series.", keyInsight: "Temporal attention creates a dynamic summary of history — different predictions attend to different past events, enabling flexible memory.", link: "https://arxiv.org/abs/1512.08756" }),
      makePaper({ id: "rnn5", title: "WaveNet: Generative Model for Raw Audio", year: 2016, difficulty: "Advanced", authors: "Van den Oord, Dieleman et al.", institution: "DeepMind", citations: 12000, section: "modern", whyItMatters: "Generated natural-sounding speech and music from raw waveforms — revolutionized text-to-speech.", problem: "Previous TTS systems used concatenative synthesis or parametric models. Can we generate raw audio sample by sample?", coreContribution: "Dilated causal convolutions that grow the receptive field exponentially — O(log n) layers for n-length context.", methodIntuition: "Generate audio one sample at a time, each influenced by thousands of previous samples via dilated convolutions — like a pyramid of increasingly wide-angle views of history.", results: "50% reduction in difference from natural speech. Adopted by Google Assistant.", strengths: "Natural audio quality, raw waveform generation, large receptive field.", limitations: "Autoregressive is slow (sample by sample); later replaced by parallel approaches.", whoShouldRead: ["researcher"], tldr: "Generate natural audio sample by sample with dilated convolutions. Revolutionized TTS.", keyInsight: "Dilated convolutions grow receptive field exponentially: layers with dilation 1,2,4,8,... cover 2^n time steps with just n layers.", link: "https://arxiv.org/abs/1609.03499" }),
      makePaper({ id: "rnn6", title: "State Space Models (S4/Mamba) for Long Sequences", year: 2022, difficulty: "Advanced", authors: "Gu, Goel, Re / Gu, Dao", institution: "Stanford / Carnegie Mellon", citations: 4000, section: "modern", whyItMatters: "SSMs handle sequences of 100K+ tokens with linear complexity — a potential transformer successor for long contexts.", problem: "Transformers are quadratic in sequence length. Can we model very long sequences efficiently?", coreContribution: "Structured state space models combining continuous-time dynamics with discrete-time computation for linear-time sequence modeling.", methodIntuition: "Instead of every token attending to every other token (quadratic), maintain a compact running state that summarizes all history — like keeping a notebook instead of rereading everything.", results: "SOTA on Long Range Arena, competitive with transformers on language while handling 100K+ contexts.", strengths: "Linear complexity, handles extremely long sequences, principled mathematical foundation.", limitations: "Newer architecture with less ecosystem support; may not match transformer quality on all tasks yet.", whoShouldRead: ["researcher"], tldr: "Linear-complexity sequence models for 100K+ tokens. The post-transformer contender.", keyInsight: "The HiPPO initialization for state matrix A is crucial — it approximates optimal history compression, enabling the state to remember relevant history indefinitely.", link: "https://arxiv.org/abs/2111.00396" }),
      makePaper({ id: "rnn7", title: "Temporal Convolutional Networks (TCN)", year: 2018, difficulty: "Intermediate", authors: "Bai, Kolter, Koltun", institution: "Carnegie Mellon / Intel", citations: 6000, section: "improvement", whyItMatters: "Showed that CNNs can outperform RNNs on sequence tasks — challenging the 'RNN for sequences' default.", problem: "Are RNNs actually the best architecture for sequences? Can simpler models work as well?", coreContribution: "1D causal dilated convolutions with residual connections for sequence modeling.", methodIntuition: "Instead of processing sequences step by step (RNN), apply convolution filters that only look backward. Stacking dilated layers gives huge receptive fields.", results: "Outperformed LSTMs on 11/13 sequence benchmarks while being simpler and more parallelizable.", strengths: "Parallelizable, stable gradients, flexible receptive field, simpler than RNNs.", limitations: "Fixed receptive field (must be large enough), no adaptive memory.", whoShouldRead: ["researcher", "interview prep"], tldr: "1D causal convolutions beat RNNs on sequences. Simpler, faster, often better.", keyInsight: "The combination of causal convolutions (can't see future) + dilation (large receptive field) + residuals (gradient flow) gives you an 'RNN replacement' that's fully parallelizable.", link: "https://arxiv.org/abs/1803.01271" }),
      makePaper({ id: "rnn8", title: "Neural ODE: Continuous-Depth Neural Networks", year: 2018, difficulty: "Advanced", authors: "Chen, Rubanova, Bettencourt, Duvenaud", institution: "University of Toronto", citations: 7000, section: "modern", whyItMatters: "Bridges neural networks and differential equations — model dynamics as continuous processes.", problem: "Standard networks have discrete layers. Can we model the transformation as a continuous ODE?", coreContribution: "Define hidden state dynamics as dh/dt = f(h,t,θ) and solve with ODE solvers. Backprop through the solver.", methodIntuition: "Instead of stacking discrete layers, let the network flow continuously like a river. The 'depth' is continuous time.", results: "Memory-efficient (constant memory via adjoint method), flexible-depth, natural for irregularly-sampled time series.", strengths: "Constant memory, handles irregular time series, elegant theory, adaptive depth.", limitations: "Slow (ODE solvers are iterative), numerical stability challenges.", whoShouldRead: ["researcher"], tldr: "Replace discrete layers with continuous dynamics. Neural networks as differential equations.", keyInsight: "The adjoint method computes gradients without storing intermediate states — O(1) memory regardless of 'depth', enabling arbitrarily deep networks.", link: "https://arxiv.org/abs/1806.07366" }),
    ],
  },
  {
    keywords: ["dropout", "regularization", "overfitting", "generalization", "batch norm", "weight decay"],
    papers: [
      makePaper({ id: "reg_t1", title: "Dropout: Preventing Neural Network Overfitting", year: 2014, difficulty: "Beginner", authors: "Srivastava, Hinton, Krizhevsky et al.", institution: "University of Toronto", citations: 45000, section: "foundation", whyItMatters: "One of the most important regularization techniques in deep learning.", problem: "Large networks memorize training data. How to regularize without heavy computation?", coreContribution: "Randomly deactivate neurons during training, forcing redundant representations.", methodIntuition: "If random team members call in sick each day, knowledge must be distributed. No single neuron becomes a bottleneck.", results: "Reduced overfitting across all architectures.", strengths: "Simple, effective, nearly universal, acts as ensemble approximation.", limitations: "Adds noise to training, slower convergence, not optimal for all architectures.", whoShouldRead: ["beginner", "interview prep"], tldr: "Turn off random neurons during training. Forces distributed knowledge.", keyInsight: "At test time, multiply weights by (1-p) to compensate — inverted dropout keeps expected magnitude consistent.", link: "https://jmlr.org/papers/v15/srivastava14a.html" }),
      makePaper({ id: "reg_t2", title: "Rethinking Generalization in Deep Learning", year: 2017, difficulty: "Advanced", authors: "Zhang, Bengio, Hardt et al.", institution: "MIT / UC Berkeley", citations: 12000, section: "foundation", whyItMatters: "Networks can memorize random labels yet generalize on real data. Challenged our understanding.", problem: "Why do overparameterized networks generalize? Classical theory says they shouldn't.", coreContribution: "Proved networks fit random labels with zero error, showing traditional complexity measures fail.", methodIntuition: "If a student can memorize a random phone book, capacity isn't the explanation. Real data has structure that guides learning.", results: "Showed regularization is neither necessary nor sufficient for generalization.", strengths: "Fundamental insight, changed research direction, well-designed experiments.", limitations: "Identifies the problem but doesn't fully solve it.", whoShouldRead: ["researcher"], tldr: "Deep nets memorize anything yet generalize on real data. Traditional theory can't explain it.", keyInsight: "Same architecture, same optimizer — real labels generalize, random labels memorize. Data structure drives generalization.", link: "https://arxiv.org/abs/1611.03530" }),
      makePaper({ id: "reg_t3", title: "Data Augmentation: A Comprehensive Survey", year: 2019, difficulty: "Beginner", authors: "Shorten, Khoshgoftaar", institution: "Florida Atlantic University", citations: 10000, section: "improvement", whyItMatters: "Often the single most effective technique for improving model performance.", problem: "Deep learning needs lots of data, but collecting is expensive. How to expand training sets?", coreContribution: "Taxonomy of augmentation: geometric, color, and advanced methods (Mixup, CutOut, AutoAugment).", methodIntuition: "A flipped cat photo is still a cat. Generate variations to teach the model what really matters.", results: "Consistent 2-10% improvements. AutoAugment learned policies beating hand-designed ones.", strengths: "Simple, universally applicable, often more impactful than architecture changes.", limitations: "Must preserve label semantics; augmentation policy design requires thought.", whoShouldRead: ["beginner", "interview prep"], tldr: "Transform data to create more examples. Often more impactful than model changes.", keyInsight: "Mixup (blend images and labels) creates smooth decision boundaries — surprisingly effective.", link: "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0" }),
      makePaper({ id: "reg_t4", title: "Early Stopping as Regularization", year: 2007, difficulty: "Beginner", authors: "Prechelt", institution: "Karlsruhe Institute of Technology", citations: 6000, section: "foundation", whyItMatters: "The simplest and most effective regularization: stop training when validation error starts increasing.", problem: "Training longer always decreases training error but eventually increases test error. When to stop?", coreContribution: "Monitor validation error and stop training when it hasn't improved for k epochs (patience).", methodIntuition: "Study until you start making more mistakes on practice tests. That's when you've peaked — studying more just means memorizing instead of learning.", results: "Standard practice in all deep learning. Often the single most important regularization technique.", strengths: "Free (no computational cost), automatic, prevents overfitting reliably.", limitations: "Requires a validation set; patience parameter needs tuning.", whoShouldRead: ["beginner", "interview prep"], tldr: "Stop when validation loss stops improving. The simplest regularization that actually works.", keyInsight: "Early stopping is mathematically equivalent to L2 regularization for linear models — but it's simpler and works for any model.", link: "https://page.mi.fu-berlin.de/prechelt/Biblio/stop_LNCS2012.pdf" }),
      makePaper({ id: "reg_t5", title: "Label Smoothing for Better Calibration", year: 2019, difficulty: "Intermediate", authors: "Müller, Kornblith, Hinton", institution: "Google Brain", citations: 5000, section: "improvement", whyItMatters: "Prevents the model from becoming overconfident — crucial for reliable predictions.", problem: "Training with hard labels (0 or 1) makes networks overconfident. How to encourage calibrated uncertainty?", coreContribution: "Replace target labels with smoothed versions: y_smooth = (1-ε)·y + ε/K where K is number of classes.", methodIntuition: "Instead of 'this is definitely a cat' (100%), say 'this is almost certainly a cat' (90%) with small probability for other classes. Teaches humility.", results: "Improved calibration, reduced overconfidence, slight accuracy gains on many benchmarks.", strengths: "Simple (one line of code), better calibration, acts as regularizer.", limitations: "Degrades nearest-neighbor-style tasks; optimal ε is dataset-dependent.", whoShouldRead: ["researcher", "interview prep"], tldr: "Soften your labels slightly. Prevents overconfidence, improves calibration.", keyInsight: "Hard labels force the logits to approach ±∞ (via softmax). Label smoothing keeps logits finite, creating a natural penalty for overconfidence.", link: "https://arxiv.org/abs/1906.02629" }),
      makePaper({ id: "reg_t6", title: "Stochastic Depth: Randomly Dropping Layers", year: 2016, difficulty: "Intermediate", authors: "Huang, Sun, Liu et al.", institution: "Cornell / Facebook", citations: 4000, section: "improvement", whyItMatters: "Dropout for layers instead of neurons — enables training of very deep networks.", problem: "Very deep ResNets suffer from diminishing returns. Can we drop entire layers during training?", coreContribution: "Randomly skip residual blocks during training; use all blocks at test time.", methodIntuition: "During training, randomly bypass entire floors of a building. The remaining floors learn to be self-sufficient. At test time, use all floors for maximum power.", results: "Trained 1202-layer ResNets successfully. Improved accuracy while reducing training time.", strengths: "Simple, reduces training time, implicit ensemble effect, enables deeper networks.", limitations: "Only works with skip connections; random depth schedule requires tuning.", whoShouldRead: ["researcher"], tldr: "Randomly skip layers during training. Deeper networks, less training time.", keyInsight: "With skip connections, dropping a layer just means passing the input through — stochastic depth creates an implicit ensemble of networks with different depths.", link: "https://arxiv.org/abs/1603.09382" }),
      makePaper({ id: "reg_t7", title: "Knowledge Distillation: Transferring Knowledge Between Models", year: 2015, difficulty: "Intermediate", authors: "Hinton, Vinyals, Dean", institution: "Google / University of Toronto", citations: 18000, section: "modern", whyItMatters: "Train a small model to mimic a large one — essential for deploying AI on edge devices.", problem: "Large models are accurate but too slow/big for deployment. How to transfer knowledge to a small model?", coreContribution: "Train a student network to match the teacher's soft probability outputs (softmax with temperature).", methodIntuition: "The teacher says: 'This is a 3, but it looks a bit like an 8 and slightly like a 5.' These soft labels carry more information than hard labels, helping the student learn richer representations.", results: "Small models achieve near-teacher accuracy at a fraction of the size and compute.", strengths: "Model compression, no architectural changes needed, captures dark knowledge.", limitations: "Requires a trained teacher, temperature tuning, student may not fully match teacher.", whoShouldRead: ["researcher", "interview prep"], tldr: "Teach a small model by imitating a large one. Soft labels carry more information than hard ones.", keyInsight: "The 'dark knowledge' is in the soft probabilities — the relative probabilities of incorrect classes contain information about learned similarities that hard labels destroy.", link: "https://arxiv.org/abs/1503.02531" }),
      makePaper({ id: "reg_t8", title: "Weight Decay and L2 Regularization: Theory and Practice", year: 1992, difficulty: "Beginner", authors: "Krogh, Hertz", institution: "Niels Bohr Institute", citations: 5000, section: "foundation", whyItMatters: "The most fundamental regularization technique — penalize large weights to prevent overfitting.", problem: "Networks with large weights are overfitting. How to encourage smaller, simpler weight values?", coreContribution: "Add λ||w||² to the loss function, pushing weights toward zero during optimization.", methodIntuition: "Tax large weights. The network can still use big weights if they're really needed, but it has to 'pay' for them — encouraging simpler solutions.", results: "Standard technique used in every ML library. Often combined with other regularization methods.", strengths: "Simple, well-understood, prevents weight explosion, equivalent to Gaussian prior.", limitations: "Uniform penalty may not suit all architectures; interaction with adaptive optimizers (see AdamW).", whoShouldRead: ["beginner"], tldr: "Penalize large weights to keep the model simple. The most basic regularization.", keyInsight: "L2 regularization is equivalent to a Gaussian prior on weights: N(0, 1/λ). Bayesian and frequentist perspectives agree on weight decay.", link: "https://papers.nips.cc/paper/1991/hash/8e296a067a37563370ded05f5a3bf3ec" }),
    ],
  },
];

export function searchPapers(query: string): Paper[] {
  const q = query.toLowerCase().trim();
  if (!q) return [];

  // Score each topic by keyword match quality
  let bestMatch: { papers: Paper[]; score: number } = { papers: [], score: 0 };

  for (const topic of allTopics) {
    let score = 0;
    for (const kw of topic.keywords) {
      if (q === kw) { score = 100; break; }
      if (q.includes(kw)) score = Math.max(score, 80);
      if (kw.includes(q)) score = Math.max(score, 70);
      // Word-level matching
      const qWords = q.split(/\s+/).filter(w => w.length > 2);
      const kwWords = kw.split(/\s+/);
      for (const qw of qWords) {
        for (const kww of kwWords) {
          if (qw === kww) score = Math.max(score, 60);
          else if (kww.includes(qw) || qw.includes(kww)) score = Math.max(score, 40);
        }
      }
    }
    if (score > bestMatch.score) {
      bestMatch = { papers: topic.papers, score };
    }
  }

  if (bestMatch.score > 0) return bestMatch.papers.map(enrichPaper);

  // Fallback: generate relevant papers
  return generateRelevantPapers(q);
}

function generateRelevantPapers(topic: string): Paper[] {
  const t = topic.charAt(0).toUpperCase() + topic.slice(1);
  const titles = [
    { title: `Foundations of ${t}: A Comprehensive Survey`, year: 2020, diff: "Beginner" as const, section: "foundation" as const },
    { title: `${t} with Deep Learning: Methods and Applications`, year: 2021, diff: "Intermediate" as const, section: "foundation" as const },
    { title: `Practical ${t}: From Theory to Real-World Deployment`, year: 2021, diff: "Intermediate" as const, section: "improvement" as const },
    { title: `Scaling ${t}: Efficient Approaches for Large Datasets`, year: 2022, diff: "Intermediate" as const, section: "improvement" as const },
    { title: `Interpretable ${t}: Understanding Model Decisions`, year: 2022, diff: "Intermediate" as const, section: "improvement" as const },
    { title: `Benchmarking ${t}: A Systematic Evaluation Framework`, year: 2022, diff: "Beginner" as const, section: "foundation" as const },
    { title: `Advances in ${t}: Modern Approaches and Future Directions`, year: 2023, diff: "Advanced" as const, section: "modern" as const },
    { title: `${t} at Scale: Industry Lessons and Best Practices`, year: 2023, diff: "Advanced" as const, section: "modern" as const },
  ];

  const impacts: ImpactClass[] = ["Foundational", "Foundational", "Production Innovation", "Optimization", "Optimization", "Foundational", "Breakthrough", "Production Innovation"];
  return titles.map((p, i) => makePaper({
    id: `gen-${topic}-${i}`,
    title: p.title,
    year: p.year,
    difficulty: p.diff,
    authors: "Various Authors",
    institution: "Various Institutions",
    citations: Math.floor(Math.random() * 5000) + 500,
    section: p.section,
    concepts: [t, "Machine Learning", i < 4 ? "Theory" : "Applied ML"],
    impact: impacts[i],
    whyItMatters: `This work provides ${i === 0 ? 'the most complete overview' : 'critical advances'} in ${t}, tracing its evolution and key developments.`,
    problem: `The ${t} literature is vast and fragmented. ${i < 3 ? 'New researchers need a unified entry point.' : 'Practitioners need practical guidance.'}`,
    coreContribution: `A ${i < 3 ? 'structured taxonomy' : 'practical framework'} of ${t} methods with clear comparisons of strengths, weaknesses, and use cases.`,
    methodIntuition: `Think of this as a ${i < 3 ? 'detailed map' : 'hands-on workshop'} for the ${t} landscape — ${i < 3 ? 'showing main paths and connections' : 'teaching you to build and deploy'}.`,
    results: `Became ${i < 3 ? 'a highly-cited reference' : 'an influential practical guide'} in the ${t} space, used in courses and industry.`,
    strengths: i < 3 ? "Comprehensive coverage, clear taxonomy, accessible writing." : "Practical focus, reproducible results, actionable guidelines.",
    limitations: i < 3 ? "Survey scope may miss very recent developments." : "Guidelines may need adaptation for specific domains.",
    tldr: `${i < 3 ? 'The definitive guide' : 'The practical handbook'} to ${t}. ${i < 3 ? 'Start here.' : 'Apply it.'}`,
    keyInsight: `The most successful ${t} approaches balance model complexity with data efficiency, finding the sweet spot for their domain.`,
    whoShouldRead: i < 3 ? ["beginner", "researcher"] : ["researcher", "interview prep"],
    link: `https://arxiv.org/search/?query=${encodeURIComponent(topic)}`,
  }));
}
